# Discerned Extraction - LLM-Enhanced Fact Extraction

## Overview

Vestig now supports two extraction modes:

1. **Naive Extraction** (default) - Fast, regex-based pattern matching
   - Extracts: file paths, SSH hosts, commands, tool usage
   - Speed: Milliseconds per session
   - Cost: Free
   - Accuracy: 80-90% for structured data

2. **Discerned Extraction** (optional) - LLM-powered semantic analysis
   - Extracts: feedback dynamics, workflows, patterns, insights
   - Speed: 10-30 seconds per session
   - Cost: ~$0.001-0.01 per session (using Haiku)
   - Accuracy: 85-95% for semantic understanding

## What Discerned Extraction Captures

### 1. Feedback Dynamics

**User → LLM:**
- `corrective` - User disagrees or corrects ("No, that's wrong")
- `clarifying` - User clarifies intent ("I meant...")
- `approving` - User confirms success ("Perfect!")
- `rejecting` - User rejects approach ("Stop")

**LLM → User:**
- `enthusiastic` - Overcomplicated solutions
- `cautious` - Excessive hedging/questions
- `corrective` - Fixing user misconceptions
- `pedagogical` - Explaining concepts
- `direct` - Just executing tasks

### 2. Session-Level Insights

- **Intent** - Overall user goal (1 sentence summary)
- **Workflow Pattern** - Emergent workflow (e.g., "verify_then_cleanup", "debug_cycle")
- **User Preferences** - Detected preferences (e.g., "prefers sshpass auth")
- **Knowledge Type** - Type of knowledge (procedural, declarative, troubleshooting)

### 3. Problem-Solution Pairs

Identifies problems encountered and their solutions:
- Links errors to fixes
- Tracks debugging cycles
- Captures resolution patterns

### 4. Pattern Detection

Cross-message patterns:
- Task sequences (A → B → C)
- Tool chains (grep → read → edit)
- Recurring workflows

## Usage

### Command Line

**Naive only (default):**
```bash
python session_parser.py session.jsonl
```

**With discerned extraction:**
```bash
python session_parser.py session.jsonl --discerned
```

**Custom model:**
```bash
python session_parser.py session.jsonl --discerned --model gpt-4o-mini
```

### Import to Memory

**Naive only:**
```bash
python agent_memory.py session.jsonl
```

**With discerned:**
```bash
python agent_memory.py session.jsonl --discerned
```

**Custom database:**
```bash
python agent_memory.py session.jsonl --discerned --db my_memory.db
```

### Python API

```python
from session_parser import ClaudeCodeSessionParser

# Naive only
parser = ClaudeCodeSessionParser()
result = parser.parse_session_file("session.jsonl")

# With discerned
parser = ClaudeCodeSessionParser(
    use_discerned=True,
    llm_model="claude-3-5-haiku-20241022"
)
result = parser.parse_session_file("session.jsonl")

# Facts are flagged with extraction_method
for fact in result['facts']:
    print(f"{fact['extraction_method']}: {fact['predicate']} = {fact['object']}")
```

## Querying by Extraction Method

```python
from agent_memory import AgentMemory

memory = AgentMemory("memory.db")

# Get only discerned facts
discerned_facts = memory.conn.execute(
    "SELECT * FROM facts WHERE extraction_method = 'discerned'"
).fetchall()

# Get feedback dynamics
feedback = memory.query_facts(predicate='user_feedback_type')

# Get workflow patterns
workflows = memory.query_facts(predicate='workflow_pattern')

# Get session intents
intents = memory.query_facts(predicate='session_intent')
```

## New Predicate Types

### Discerned-Only Predicates

These are ONLY generated by discerned extraction:

- `user_feedback_type` - User's feedback style
- `assistant_tone` - LLM's response tone
- `session_intent` - Overall session goal
- `workflow_pattern` - Detected workflow
- `user_preference` - User preference detected
- `problem_description` - Problem identified
- `solution_found` - Solution to problem
- `knowledge_type` - Type of knowledge exchanged

### Naive Predicates (still present)

- `used_tool` - Tool usage
- `executed_command` - Bash commands
- `connects_to_host` - SSH connections
- `mentions_path` - File paths
- `targets_system` - Systems mentioned
- `operation_type` - Operation classification

## Cost & Performance

### Naive Extraction
- Time: ~50ms per session
- Cost: $0
- Facts: 50-200 per session
- Confidence: 0.7-1.0

### Discerned Extraction (Haiku 3.5)
- Time: ~10-30s per session
- Cost: ~$0.001-0.01 per session
- Facts: 5-20 per session
- Confidence: 0.8-0.9

### Recommendation

**Use naive for:**
- Batch processing many sessions
- Structured data extraction
- Real-time parsing
- Cost-sensitive applications

**Use discerned for:**
- Important sessions you want deep insights from
- Learning user preferences
- Understanding conversation dynamics
- Building high-quality training data

**Hybrid approach:**
- Run naive on all sessions
- Run discerned on 10% sample or important sessions
- 90% of value, 10% of cost

## Requirements

### For Naive Extraction
```bash
# No dependencies beyond Python stdlib
```

### For Discerned Extraction
```bash
# Install Simon Willison's llm CLI
pip install llm

# Configure Claude (or other provider)
llm keys set claude
# Enter your API key when prompted

# Test it works
llm -m claude-3-5-haiku-20241022 "Say hello"
```

### Alternative Models

**Claude (via llm):**
- `claude-3-5-haiku-20241022` (recommended: fast + cheap)
- `claude-3-5-sonnet-20241022` (better quality, slower)

**OpenAI (via llm):**
- `gpt-4o-mini` (fast + cheap)
- `gpt-4o` (better quality, more expensive)

## Database Schema

The `extraction_method` field is now stored in the facts table:

```sql
CREATE TABLE facts (
    id INTEGER PRIMARY KEY,
    subject TEXT NOT NULL,
    predicate TEXT NOT NULL,
    object TEXT,
    value_type TEXT,
    confidence REAL,
    source_session TEXT,
    timestamp TEXT,
    context TEXT,
    extraction_method TEXT DEFAULT 'naive',  -- 'naive' or 'discerned'
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
)
```

## Example Output

### Naive Facts
```json
{
  "subject": "action_a1b2c3d4",
  "predicate": "connects_to_host",
  "object": "192.168.20.4",
  "confidence": 0.9,
  "extraction_method": "naive"
}
```

### Discerned Facts
```json
{
  "subject": "session_xyz",
  "predicate": "workflow_pattern",
  "object": "verify_then_cleanup",
  "confidence": 0.85,
  "extraction_method": "discerned",
  "context": "User consistently verified archives before deletion"
}
```

## Troubleshooting

### "llm command not found"
```bash
pip install llm
```

### "No API key configured"
```bash
llm keys set claude
# or
llm keys set openai
```

### LLM call timeout
Increase timeout in `discerned_extractor.py`:
```python
result = subprocess.run(
    ['llm', '-m', self.model],
    input=prompt,
    capture_output=True,
    text=True,
    timeout=60  # Increase from 30 to 60 seconds
)
```

### JSON parsing errors
The LLM sometimes returns malformed JSON. This is handled gracefully with error messages. If persistent, try a more capable model like Sonnet.

## Future Enhancements

- [ ] Batch processing for multiple sessions
- [ ] Caching LLM responses
- [ ] Confidence calibration based on model
- [ ] Cross-session pattern mining
- [ ] Temporal reasoning (fact decay)
- [ ] Contradiction detection
