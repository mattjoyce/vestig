src/vestig/__init__.py
---
"""Vestig: LLM Agent Memory System"""

__version__ = "0.1.0"


---
src/vestig/core/__init__.py
---
"""Core functionality for Vestig memory system"""


---
src/vestig/core/cli.py
---
"""CLI entry point for Vestig memory system"""

import argparse
import glob
import json
import os
import sys
from importlib.metadata import PackageNotFoundError, version
from typing import Any

from vestig.core.commitment import commit_memory
from vestig.core.config import load_config
from vestig.core.embeddings import EmbeddingEngine
from vestig.core.event_storage import MemoryEventStorage
from vestig.core.storage import MemoryStorage
from vestig.core.tracerank import TraceRankConfig


def get_version() -> str:
    """Get the installed version of vestig."""
    try:
        return version("vestig")
    except PackageNotFoundError:
        return "dev"


def validate_config(config: dict[str, Any]) -> None:
    """
    Validate required config keys.

    Args:
        config: Configuration dictionary

    Raises:
        ValueError: If required keys are missing
    """
    required_keys = [
        ("embedding", "model"),
        ("embedding", "dimension"),
        ("embedding", "normalize"),
        ("storage", "db_path"),
    ]

    for *path, key in required_keys:
        current = config
        for part in path:
            if part not in current:
                raise ValueError(f"Missing required config key: {'.'.join(path)}.{key}")
            current = current[part]
        if key not in current:
            raise ValueError(f"Missing required config key: {'.'.join(path)}.{key}")


def _truncate(text: str, length: int = 80) -> str:
    if len(text) <= length:
        return text
    return text[: max(0, length - 3)] + "..."


def _resolve_node_label(storage: MemoryStorage, node_id: str, length: int = 80) -> str:
    if node_id.startswith("mem_"):
        memory = storage.get_memory(node_id)
        if memory:
            return _truncate(memory.content, length)
    if node_id.startswith("ent_"):
        entity = storage.get_entity(node_id)
        if entity:
            return f"{entity.canonical_name} ({entity.entity_type})"
    return node_id


def expand_ingest_paths(pattern: str, recursive: bool = False) -> list[str]:
    """Expand glob patterns for ingest paths.

    Args:
        pattern: File path or glob pattern (e.g., "*.txt", "**/*.md")
        recursive: Enable recursive globbing (allows ** to match subdirectories)

    Returns:
        List of matching file paths
    """
    expanded = os.path.expanduser(pattern)
    if glob.has_magic(expanded):
        return sorted(glob.glob(expanded, recursive=recursive))
    return [expanded]


def build_runtime(
    config: dict[str, Any],
) -> tuple[MemoryStorage, EmbeddingEngine, MemoryEventStorage, TraceRankConfig]:
    """
    Build storage, embedding engine, event storage, and TraceRank config from config.

    Args:
        config: Configuration dictionary

    Returns:
        Tuple of (storage, embedding_engine, event_storage, tracerank_config)
    """
    embedding_engine = EmbeddingEngine(
        model_name=config["embedding"]["model"],
        expected_dimension=config["embedding"]["dimension"],
        normalize=config["embedding"]["normalize"],
        provider=config["embedding"].get("provider", "llm"),  # Default to llm
        max_length=config["embedding"].get("max_length"),  # Optional truncation
    )
    storage = MemoryStorage(config["storage"]["db_path"])
    event_storage = MemoryEventStorage(storage.conn)  # M3: Share DB connection

    # M3: Build TraceRank config
    m3_config = config.get("m3", {})
    tracerank_cfg = m3_config.get("tracerank", {})
    tracerank_config = TraceRankConfig(
        enabled=tracerank_cfg.get("enabled", True),
        tau_days=tracerank_cfg.get("tau_days", 21.0),
        cooldown_hours=tracerank_cfg.get("cooldown_hours", 24.0),
        burst_discount=tracerank_cfg.get("burst_discount", 0.2),
        k=tracerank_cfg.get("k", 0.35),
        ephemeral_tau_days=tracerank_cfg.get("ephemeral_tau_days"),
    )

    return storage, embedding_engine, event_storage, tracerank_config


def cmd_add(args):
    """Handle 'vestig memory add' command"""
    config = args.config_dict
    storage, embedding_engine, event_storage, _ = build_runtime(config)

    # Parse tags if provided
    tags = None
    if args.tags:
        tags = [tag.strip() for tag in args.tags.split(",")]

    try:
        outcome = commit_memory(
            content=args.content,
            storage=storage,
            embedding_engine=embedding_engine,
            source=args.source,
            hygiene_config=config.get("hygiene", {}),
            tags=tags,
            event_storage=event_storage,  # M3: Enable event logging
            m4_config=config.get("m4", {}),  # M4: Enable one-shot entity extraction
        )

        # Display outcome info
        if outcome.outcome == "EXACT_DUPE":
            print(f"Memory stored: {outcome.memory_id} (exact duplicate detected)")
        elif outcome.outcome == "NEAR_DUPE":
            print(
                f"Memory stored: {outcome.memory_id} "
                f"(near-duplicate of {outcome.matched_memory_id}, "
                f"score={outcome.query_score:.4f})"
            )
        else:  # INSERTED_NEW
            print(f"Memory stored: {outcome.memory_id}")

    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        storage.close()


def cmd_search(args):
    """Handle 'vestig memory search' command"""
    from vestig.core.retrieval import format_search_results, search_memories

    config = args.config_dict
    storage, embedding_engine, event_storage, tracerank_config = build_runtime(config)

    try:
        results = search_memories(
            query=args.query,
            storage=storage,
            embedding_engine=embedding_engine,
            limit=args.limit,
            event_storage=event_storage,
            tracerank_config=tracerank_config,
            show_timing=getattr(args, 'timing', False),
        )
        print(format_search_results(results))
    finally:
        storage.close()


def cmd_recall(args):
    """Handle 'vestig memory recall' command"""
    from vestig.core.retrieval import (
        format_recall_results,
        format_recall_results_with_explanation,
        search_memories,
    )

    config = args.config_dict
    storage, embedding_engine, event_storage, tracerank_config = build_runtime(config)

    try:
        results = search_memories(
            query=args.query,
            storage=storage,
            embedding_engine=embedding_engine,
            limit=args.limit,
            event_storage=event_storage,
            tracerank_config=tracerank_config,
            show_timing=getattr(args, 'timing', False),
        )

        # Format with or without explanation
        if args.explain:
            output = format_recall_results_with_explanation(
                results, event_storage, storage, tracerank_config
            )
        else:
            output = format_recall_results(results)

        print(output)
    finally:
        storage.close()


def cmd_show(args):
    """Handle 'vestig memory show' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        if args.expand > 1:
            print("Error: Only expansion depth 0 or 1 is supported", file=sys.stderr)
            sys.exit(1)

        memory = storage.get_memory(args.id)
        if memory is None:
            print(f"Error: Memory not found: {args.id}", file=sys.stderr)
            sys.exit(1)

        # Display full memory details
        print(f"ID: {memory.id}")
        print(f"Created: {memory.created_at}")
        print(f"t_valid: {memory.t_valid}")
        print(f"t_invalid: {memory.t_invalid}")
        print(f"t_created: {memory.t_created}")
        print(f"t_expired: {memory.t_expired}")
        print(f"temporal_stability: {memory.temporal_stability}")
        print(f"last_seen_at: {memory.last_seen_at}")
        print(f"reinforce_count: {memory.reinforce_count}")
        print(f"Metadata: {json.dumps(memory.metadata, indent=2)}")
        print(f"Embedding: {len(memory.content_embedding)} dimensions")
        print(f"  First 5 values: {memory.content_embedding[:5]}")
        print(f"\nContent:\n{memory.content}")

        if args.expand == 1:
            mentions = storage.get_edges_from_memory(
                memory.id,
                edge_type="MENTIONS",
                include_expired=args.include_expired,
            )
            related = storage.get_edges_from_memory(
                memory.id,
                edge_type="RELATED",
                include_expired=args.include_expired,
            )

            print("\nMENTIONS:")
            if not mentions:
                print("  (none)")
            for edge in mentions:
                label = _resolve_node_label(storage, edge.to_node)
                conf = f"{edge.confidence:.2f}" if edge.confidence is not None else "n/a"
                print(f"  - {edge.to_node} | {label} | confidence={conf}")

            print("\nRELATED:")
            if not related:
                print("  (none)")
            for edge in related:
                label = _resolve_node_label(storage, edge.to_node)
                print(f"  - {edge.to_node} | {label} | weight={edge.weight:.2f}")
    finally:
        storage.close()


def cmd_memory_list(args):
    """Handle 'vestig memory list' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        query = "SELECT id, content, created_at, t_expired, metadata FROM memories "
        params = []
        if not args.include_expired:
            query += "WHERE t_expired IS NULL "
        query += "ORDER BY created_at DESC LIMIT ?"
        params.append(args.limit)

        cursor = storage.conn.execute(query, params)
        rows = cursor.fetchall()

        for row in rows:
            memory_id, content, created_at, t_expired, metadata_json = row
            metadata = json.loads(metadata_json)
            source = metadata.get("source", "unknown")
            status = "expired" if t_expired else "active"
            snippet = _truncate(content, args.snippet_len)
            print(f"{memory_id} | {created_at} | {source} | {status} | {snippet}")
    finally:
        storage.close()


def cmd_entity_list(args):
    """Handle 'vestig entity list' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        query = (
            "SELECT e.id, e.entity_type, e.canonical_name, e.created_at, "
            "e.expired_at, e.merged_into, "
            "COUNT(ed.edge_id) AS mentions "
            "FROM entities e "
            "LEFT JOIN edges ed ON ed.to_node = e.id "
            "AND ed.edge_type = 'MENTIONS' AND ed.t_expired IS NULL "
        )
        params = []
        if not args.include_expired:
            query += "WHERE e.expired_at IS NULL "
        query += "GROUP BY e.id ORDER BY mentions DESC, e.created_at DESC LIMIT ?"
        params.append(args.limit)

        cursor = storage.conn.execute(query, params)
        rows = cursor.fetchall()
        for row in rows:
            entity_id, entity_type, name, created_at, expired_at, merged_into, mentions = row
            status = "expired" if expired_at else "active"
            merged = f" -> {merged_into}" if merged_into else ""
            print(
                f"{entity_id} | {entity_type} | {name} | mentions={mentions} | "
                f"{created_at} | {status}{merged}"
            )
    finally:
        storage.close()


def cmd_entity_show(args):
    """Handle 'vestig entity show' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        if args.expand > 1:
            print("Error: Only expansion depth 0 or 1 is supported", file=sys.stderr)
            sys.exit(1)

        entity = storage.get_entity(args.id)
        if entity is None:
            print(f"Error: Entity not found: {args.id}", file=sys.stderr)
            sys.exit(1)

        print(f"ID: {entity.id}")
        print(f"Type: {entity.entity_type}")
        print(f"Name: {entity.canonical_name}")
        print(f"Created: {entity.created_at}")
        print(f"Expired: {entity.expired_at}")
        print(f"Merged into: {entity.merged_into}")

        if args.expand == 1:
            edges = storage.get_edges_to_entity(
                entity.id,
                include_expired=args.include_expired,
            )
            print("\nMENTIONED IN:")
            if not edges:
                print("  (none)")
            for edge in edges:
                label = _resolve_node_label(storage, edge.from_node)
                conf = f"{edge.confidence:.2f}" if edge.confidence is not None else "n/a"
                print(f"  - {edge.from_node} | {label} | confidence={conf}")
    finally:
        storage.close()


def cmd_edge_list(args):
    """Handle 'vestig edge list' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        query = (
            "SELECT edge_id, from_node, to_node, edge_type, weight, confidence, "
            "t_created, t_expired FROM edges "
        )
        params = []
        if args.type != "ALL":
            query += "WHERE edge_type = ? "
            params.append(args.type)
            if not args.include_expired:
                query += "AND t_expired IS NULL "
        else:
            if not args.include_expired:
                query += "WHERE t_expired IS NULL "
        query += "ORDER BY t_created DESC LIMIT ?"
        params.append(args.limit)

        cursor = storage.conn.execute(query, params)
        rows = cursor.fetchall()
        for row in rows:
            edge_id, from_node, to_node, edge_type, weight, confidence, t_created, t_expired = row
            from_label = _resolve_node_label(storage, from_node, args.snippet_len)
            to_label = _resolve_node_label(storage, to_node, args.snippet_len)
            conf = f"{confidence:.2f}" if confidence is not None else "n/a"
            status = "expired" if t_expired else "active"
            print(
                f"{edge_id} | {edge_type} | {status} | {from_node} -> {to_node} | "
                f"weight={weight:.2f} | confidence={conf}"
            )
            print(f"  from: {from_label}")
            print(f"  to:   {to_label}")
    finally:
        storage.close()


def cmd_edge_show(args):
    """Handle 'vestig edge show' command"""
    config = args.config_dict
    storage = MemoryStorage(config["storage"]["db_path"])

    try:
        edge = storage.get_edge(args.id)
        if edge is None:
            print(f"Error: Edge not found: {args.id}", file=sys.stderr)
            sys.exit(1)

        print(f"ID: {edge.edge_id}")
        print(f"Type: {edge.edge_type}")
        print(f"From: {edge.from_node}")
        print(f"To: {edge.to_node}")
        print(f"Weight: {edge.weight}")
        print(f"Confidence: {edge.confidence}")
        print(f"Evidence: {edge.evidence}")
        print(f"t_valid: {edge.t_valid}")
        print(f"t_invalid: {edge.t_invalid}")
        print(f"t_created: {edge.t_created}")
        print(f"t_expired: {edge.t_expired}")

        from_label = _resolve_node_label(storage, edge.from_node)
        to_label = _resolve_node_label(storage, edge.to_node)
        print(f"\nFrom node: {from_label}")
        print(f"To node: {to_label}")
    finally:
        storage.close()


def cmd_deprecate(args):
    """Handle 'vestig memory deprecate' command"""
    from vestig.core.models import EventNode

    config = args.config_dict
    storage, _, event_storage, _ = build_runtime(config)

    try:
        # Verify memory exists
        memory = storage.get_memory(args.id)
        if memory is None:
            print(f"Error: Memory not found: {args.id}", file=sys.stderr)
            sys.exit(1)

        # Check if already deprecated
        if memory.t_expired:
            print(
                f"Warning: Memory {args.id} is already deprecated (t_expired={memory.t_expired})",
                file=sys.stderr,
            )
            sys.exit(1)

        # M3 FIX #5: Atomic transaction for event + deprecation
        with storage.conn:
            # Create DEPRECATE event
            event = EventNode.create(
                memory_id=args.id,
                event_type="DEPRECATE",
                source="manual",
                payload={
                    "reason": args.reason or "Manual deprecation",
                    "t_invalid": args.t_invalid,
                },
            )
            event_storage.add_event(event)

            # Mark memory as deprecated
            storage.deprecate_memory(args.id, t_invalid=args.t_invalid)

            # Transaction commits here automatically

        print(f"Memory {args.id} deprecated successfully")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        storage.close()


def cmd_regen_embeddings(args):
    """Handle 'vestig memory regen-embeddings' command"""
    import json

    config = args.config_dict

    # Override embedding config with new model if specified
    if args.model:
        config["embedding"]["model"] = args.model
        print(f"Using model: {args.model}")

    # Build runtime with new embedding engine
    storage, embedding_engine, _, _ = build_runtime(config)

    try:
        # Get all memories (includes both MEMORY and SUMMARY kinds)
        print("Loading all memories from database...")
        all_memories = storage.get_all_memories()

        # Apply limit if specified (for testing)
        if args.limit:
            all_memories = all_memories[:args.limit]
            print(f"Processing first {len(all_memories)} memories (--limit {args.limit})")
        else:
            print(f"Processing {len(all_memories)} memories")

        if not all_memories:
            print("No memories found in database")
            return

        # Process memories in batches
        batch_size = args.batch_size
        total_processed = 0
        total_errors = 0

        for i in range(0, len(all_memories), batch_size):
            batch = all_memories[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(all_memories) + batch_size - 1) // batch_size

            print(f"\nProcessing batch {batch_num}/{total_batches} ({len(batch)} memories)...")

            # Generate embeddings for batch
            texts = [m.content for m in batch]
            try:
                embeddings = embedding_engine.embed_batch(texts)
            except Exception as e:
                print(f"Error generating embeddings for batch {batch_num}: {e}", file=sys.stderr)
                total_errors += len(batch)
                continue

            # Update database with new embeddings
            with storage.conn:
                for memory, embedding in zip(batch, embeddings):
                    try:
                        embedding_json = json.dumps(embedding)
                        storage.conn.execute(
                            "UPDATE memories SET content_embedding = ? WHERE id = ?",
                            (embedding_json, memory.id)
                        )
                        total_processed += 1
                    except Exception as e:
                        print(f"Error updating memory {memory.id}: {e}", file=sys.stderr)
                        total_errors += 1

            # Show progress
            progress_pct = (i + len(batch)) / len(all_memories) * 100
            print(f"Progress: {total_processed}/{len(all_memories)} ({progress_pct:.1f}%)")

        print(f"\n{'='*60}")
        print("REGENERATION COMPLETE")
        print(f"{'='*60}")
        print(f"Total processed: {total_processed}")
        print(f"Total errors: {total_errors}")
        print(f"Model: {config['embedding']['model']}")
        print(f"Dimension: {config['embedding']['dimension']}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        storage.close()


def cmd_memory(args):
    """Handle noun subcommand routing"""
    # If we get here without a subcommand, show help
    parser = getattr(args, "noun_parser", None)
    if parser is None:
        parser = getattr(args, "memory_parser", None)
    if parser is not None:
        parser.print_help()
    sys.exit(1)


def cmd_ingest(args):
    """Handle 'vestig ingest' command"""
    from vestig.core.ingestion import ingest_document

    config = args.config_dict
    storage, embedding_engine, event_storage, _ = build_runtime(config)

    # Get M4 config for entity extraction
    m4_config = config.get("m4", {})

    # Get prompts config for prompt version selection
    prompts_config = config.get("prompts", {})

    # Get ingestion config with CLI overrides
    ingestion_config = config.get("ingestion", {})
    model = args.model if args.model else ingestion_config.get("model")
    chunk_size = args.chunk_size if args.chunk_size else ingestion_config.get("chunk_size", 20000)
    chunk_overlap = (
        args.chunk_overlap if args.chunk_overlap else ingestion_config.get("chunk_overlap", 500)
    )
    min_confidence = (
        args.min_confidence
        if args.min_confidence is not None
        else ingestion_config.get("min_confidence", 0.6)
    )
    source_format = args.format if args.format else ingestion_config.get("format", "auto")
    format_config = ingestion_config.get("claude_session", {})
    force_entities = ingestion_config.get("force_entities", [])
    if args.force_entity:
        force_entities = force_entities + args.force_entity

    if not model:
        raise ValueError(
            "Model must be specified in config (ingestion.model) or via --model argument"
        )

    paths = expand_ingest_paths(args.document, recursive=args.recurse)
    if not paths:
        print(f"No files match: {args.document}", file=sys.stderr)
        sys.exit(1)

    total = {
        "chunks": 0,
        "extracted": 0,
        "committed": 0,
        "deduped": 0,
        "entities": 0,
        "errors": 0,
    }
    failures = []

    for idx, document_path in enumerate(paths, 1):
        try:
            if len(paths) > 1:
                print("\n" + "=" * 70)
                print(f"INGESTING {idx}/{len(paths)}")
                print("=" * 70)

            result = ingest_document(
                document_path=document_path,
                storage=storage,
                embedding_engine=embedding_engine,
                event_storage=event_storage,
                m4_config=m4_config,
                prompts_config=prompts_config,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                extraction_model=model,
                min_confidence=min_confidence,
                source="document_ingest",
                source_format=source_format,
                format_config=format_config,
                force_entities=force_entities,
                verbose=args.verbose,
            )

            total["chunks"] += result.chunks_processed
            total["extracted"] += result.memories_extracted
            total["committed"] += result.memories_committed
            total["deduped"] += result.memories_deduplicated
            total["entities"] += result.entities_created
            total["errors"] += len(result.errors)

            # Print summary
            print("\n" + "=" * 70)
            print("INGESTION COMPLETE")
            print("=" * 70)
            print(f"Document: {result.document_path}")
            print(f"Chunks processed: {result.chunks_processed}")
            print(f"Memories extracted: {result.memories_extracted}")
            print(f"Memories committed: {result.memories_committed}")
            print(f"Duplicates skipped: {result.memories_deduplicated}")
            print(f"Entities created: {result.entities_created}")

            if result.errors:
                print(f"\nErrors: {len(result.errors)}")
                for error in result.errors[:5]:  # Show first 5 errors
                    print(f"  - {error}")
                if len(result.errors) > 5:
                    print(f"  ... and {len(result.errors) - 5} more")

            print("=" * 70)

        except FileNotFoundError as e:
            failures.append(str(e))
        except Exception as e:
            failures.append(f"{document_path}: {e}")

    if len(paths) > 1:
        print("\n" + "=" * 70)
        print("INGESTION SUMMARY")
        print("=" * 70)
        print(f"Documents: {len(paths)}")
        print(f"Chunks processed: {total['chunks']}")
        print(f"Memories extracted: {total['extracted']}")
        print(f"Memories committed: {total['committed']}")
        print(f"Duplicates skipped: {total['deduped']}")
        print(f"Entities created: {total['entities']}")
        print(f"Errors: {total['errors']}")
        print("=" * 70)

    if failures:
        print("\nErrors during ingestion:", file=sys.stderr)
        for error in failures[:5]:
            print(f"  - {error}", file=sys.stderr)
        if len(failures) > 5:
            print(f"  ... and {len(failures) - 5} more", file=sys.stderr)
        sys.exit(1)


def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        prog="vestig",
        description="Vestig: LLM Agent Memory System",
    )
    parser.add_argument(
        "--config",
        default="config.yaml",
        help="Path to config file (default: config.yaml)",
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # vestig ingest
    parser_ingest = subparsers.add_parser(
        "ingest", help="Ingest document by extracting memories with LLM"
    )
    parser_ingest.add_argument("document", help="Path to document file or glob pattern")
    parser_ingest.add_argument(
        "-r",
        "--recurse",
        action="store_true",
        help="Enable recursive globbing (allows ** to match subdirectories)",
    )
    parser_ingest.add_argument(
        "--format",
        choices=["auto", "plain", "claude-session"],
        help="Input format (default from config ingestion.format or auto)",
    )
    parser_ingest.add_argument(
        "--force-entity",
        action="append",
        help="Force entity on every memory (format TYPE:Name, repeatable)",
    )
    parser_ingest.add_argument(
        "--chunk-size",
        type=int,
        help="Characters per chunk (overrides config, default from config or 20000)",
    )
    parser_ingest.add_argument(
        "--chunk-overlap",
        type=int,
        help="Character overlap between chunks (overrides config, default from config or 500)",
    )
    parser_ingest.add_argument(
        "--model",
        help="LLM model for extraction (overrides config, required if not in config)",
    )
    parser_ingest.add_argument(
        "--min-confidence",
        type=float,
        help="Minimum confidence for extracted memories (overrides config, default from config or 0.6)",
    )
    parser_ingest.add_argument(
        "--verbose",
        action="store_true",
        help="Show detailed extraction output (memories, entities, confidence values)",
    )
    parser_ingest.set_defaults(func=cmd_ingest)

    # vestig memory
    parser_memory = subparsers.add_parser("memory", help="Memory operations")
    memory_subparsers = parser_memory.add_subparsers(dest="memory_command", help="Memory commands")

    # vestig memory add
    parser_add = memory_subparsers.add_parser("add", help="Add a new memory")
    parser_add.add_argument("content", help="Memory content")
    parser_add.add_argument("--source", default="manual", help="Memory source (default: manual)")
    parser_add.add_argument("--tags", help="Comma-separated tags (e.g., bug,auth,fix)")
    parser_add.set_defaults(func=cmd_add)

    # vestig memory search
    parser_search = memory_subparsers.add_parser(
        "search", help="Search memories by semantic similarity"
    )
    parser_search.add_argument("query", help="Search query")
    parser_search.add_argument(
        "--limit", type=int, default=5, help="Number of results (default: 5)"
    )
    parser_search.add_argument(
        "--timing",
        action="store_true",
        help="Show performance timing breakdown",
    )
    parser_search.set_defaults(func=cmd_search)

    # vestig memory recall
    parser_recall = memory_subparsers.add_parser(
        "recall", help="Recall memories formatted for agent context"
    )
    parser_recall.add_argument("query", help="Recall query")
    parser_recall.add_argument(
        "--limit", type=int, default=5, help="Number of results (default: 5)"
    )
    parser_recall.add_argument(
        "--explain",
        action="store_true",
        help="Show explanation for why each memory was retrieved",
    )
    parser_recall.add_argument(
        "--timing",
        action="store_true",
        help="Show performance timing breakdown",
    )
    parser_recall.set_defaults(func=cmd_recall)

    # vestig memory show
    parser_show = memory_subparsers.add_parser("show", help="Show memory details by ID")
    parser_show.add_argument("id", help="Memory ID")
    parser_show.add_argument(
        "--expand",
        type=int,
        default=0,
        help="Expansion depth (0 or 1)",
    )
    parser_show.add_argument(
        "--include-expired",
        action="store_true",
        help="Include expired edges in expansion",
    )
    parser_show.set_defaults(func=cmd_show)

    # vestig memory list
    parser_list = memory_subparsers.add_parser("list", help="List memories")
    parser_list.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of memories to show (default: 20)",
    )
    parser_list.add_argument(
        "--snippet-len",
        type=int,
        default=80,
        help="Snippet length for content preview (default: 80)",
    )
    parser_list.add_argument(
        "--include-expired",
        action="store_true",
        help="Include expired memories",
    )
    parser_list.set_defaults(func=cmd_memory_list)

    # vestig memory deprecate
    parser_deprecate = memory_subparsers.add_parser("deprecate", help="Mark memory as deprecated")
    parser_deprecate.add_argument("id", help="Memory ID to deprecate")
    parser_deprecate.add_argument(
        "--reason", help="Reason for deprecation (stored in event payload)"
    )
    parser_deprecate.add_argument(
        "--t-invalid", help="When fact became invalid (ISO 8601 timestamp)"
    )
    parser_deprecate.set_defaults(func=cmd_deprecate)

    # vestig memory regen-embeddings
    parser_regen = memory_subparsers.add_parser(
        "regen-embeddings", help="Regenerate all embeddings with a new model"
    )
    parser_regen.add_argument(
        "--model",
        help="New embedding model to use (overrides config, e.g., 'ollama/nomic-embed-text')",
    )
    parser_regen.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Number of memories to process per batch (default: 100)",
    )
    parser_regen.add_argument(
        "--limit",
        type=int,
        help="Only regenerate first N memories (for testing)",
    )
    parser_regen.set_defaults(func=cmd_regen_embeddings)

    # Set default for memory command (show help if no subcommand)
    parser_memory.set_defaults(func=cmd_memory, noun_parser=parser_memory)

    # vestig entity
    parser_entity = subparsers.add_parser("entity", help="Entity operations")
    entity_subparsers = parser_entity.add_subparsers(dest="entity_command", help="Entity commands")

    parser_entity_list = entity_subparsers.add_parser("list", help="List entities")
    parser_entity_list.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of entities to show (default: 20)",
    )
    parser_entity_list.add_argument(
        "--include-expired",
        action="store_true",
        help="Include expired entities",
    )
    parser_entity_list.set_defaults(func=cmd_entity_list)

    parser_entity_show = entity_subparsers.add_parser("show", help="Show entity details by ID")
    parser_entity_show.add_argument("id", help="Entity ID")
    parser_entity_show.add_argument(
        "--expand",
        type=int,
        default=0,
        help="Expansion depth (0 or 1)",
    )
    parser_entity_show.add_argument(
        "--include-expired",
        action="store_true",
        help="Include expired edges in expansion",
    )
    parser_entity_show.set_defaults(func=cmd_entity_show)

    parser_entity.set_defaults(func=cmd_memory, noun_parser=parser_entity)

    # vestig edge
    parser_edge = subparsers.add_parser("edge", help="Edge operations")
    edge_subparsers = parser_edge.add_subparsers(dest="edge_command", help="Edge commands")

    parser_edge_list = edge_subparsers.add_parser("list", help="List edges")
    parser_edge_list.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of edges to show (default: 20)",
    )
    parser_edge_list.add_argument(
        "--type",
        choices=["ALL", "MENTIONS", "RELATED"],
        default="ALL",
        help="Edge type filter (default: ALL)",
    )
    parser_edge_list.add_argument(
        "--snippet-len",
        type=int,
        default=60,
        help="Snippet length for node previews (default: 60)",
    )
    parser_edge_list.add_argument(
        "--include-expired",
        action="store_true",
        help="Include expired edges",
    )
    parser_edge_list.set_defaults(func=cmd_edge_list)

    parser_edge_show = edge_subparsers.add_parser("show", help="Show edge details by ID")
    parser_edge_show.add_argument("id", help="Edge ID")
    parser_edge_show.set_defaults(func=cmd_edge_show)

    parser_edge.set_defaults(func=cmd_memory, noun_parser=parser_edge)

    args = parser.parse_args()

    # Show help if no command provided
    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Load config with minimal error handling (fail hard on other errors)
    try:
        config = load_config(args.config)
        validate_config(config)
        args.config_dict = config
    except FileNotFoundError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

    # Display version if not suppressed
    display_config = config.get("display", {})
    if display_config.get("show_version", True):
        print(f"vestig v{get_version()}")

    # Dispatch to subcommand handler
    args.func(args)


if __name__ == "__main__":
    main()


---
src/vestig/core/commitment.py
---
"""Commitment pipeline with M2 quality firewall"""

import hashlib
import re
import uuid
from typing import TYPE_CHECKING, Any

from vestig.core.embeddings import EmbeddingEngine
from vestig.core.events import CommitOutcome, OnCommitHook
from vestig.core.models import MemoryNode
from vestig.core.storage import MemoryStorage

if TYPE_CHECKING:
    from vestig.core.event_storage import MemoryEventStorage

# Default hygiene settings (used if not in config)
DEFAULT_HYGIENE = {
    "min_chars": 12,
    "max_chars": 4000,
    "normalize_whitespace": True,
    "reject_exact_duplicates": True,
    "near_duplicate": {
        "enabled": True,
        "threshold": 0.92,
        "skip_manual_source": True,  # M3 FIX #6: Skip expensive scan for CLI adds
    },
}


def normalize_content(content: str, normalize_whitespace: bool = True) -> str:
    """
    Normalize content for storage.

    Args:
        content: Raw content string
        normalize_whitespace: Whether to normalize whitespace

    Returns:
        Normalized content string
    """
    # Strip leading/trailing whitespace
    normalized = content.strip()

    if normalize_whitespace:
        # Collapse multiple spaces/newlines
        normalized = re.sub(r"\s+", " ", normalized)

    return normalized


def validate_content(content: str, hygiene_config: dict[str, Any]) -> None:
    """
    Validate content against hygiene rules.

    Args:
        content: Normalized content string
        hygiene_config: Hygiene configuration dict

    Raises:
        ValueError: If content fails validation
    """
    # Check minimum length
    min_chars = hygiene_config.get("min_chars", DEFAULT_HYGIENE["min_chars"])
    if len(content) < min_chars:
        raise ValueError(f"Content too short: {len(content)} chars (minimum: {min_chars})")

    # Check maximum length
    max_chars = hygiene_config.get("max_chars", DEFAULT_HYGIENE["max_chars"])
    if len(content) > max_chars:
        raise ValueError(
            f"Content too long: {len(content)} chars (maximum: {max_chars}). "
            f"Consider splitting or summarizing."
        )

    # Reject obviously useless content (simple rules)
    useless_patterns = [
        r"^(ok|okay|thanks|thx|lol|haha|yeah|yep|nope|k)$",
        r"^\.+$",  # Just dots
        r"^\s*$",  # Just whitespace (shouldn't happen after normalize)
    ]

    content_lower = content.lower()
    for pattern in useless_patterns:
        if re.match(pattern, content_lower):
            raise ValueError(f"Content rejected: appears to be non-substantive ('{content[:50]}')")


def commit_memory(
    content: str,
    storage: MemoryStorage,
    embedding_engine: EmbeddingEngine,
    source: str = "manual",
    hygiene_config: dict[str, Any] = None,
    tags: list[str] = None,
    artifact_ref: str | None = None,
    on_commit: OnCommitHook | None = None,
    event_storage: MemoryEventStorage | None = None,  # M3: Event logging
    m4_config: dict[str, Any] | None = None,  # M4: Graph config
    pre_extracted_entities: list[tuple[str, str, float, str]] | None = None,  # M4: Pre-extracted entities
    temporal_hints: Any | None = None,  # ExtractedMemory with temporal fields
) -> CommitOutcome:
    """
    Commit a memory to storage with M2 quality firewall, M3 event logging, M4 entity extraction, and temporal hints.

    Args:
        content: Memory content text
        storage: Storage instance
        embedding_engine: Embedding engine instance
        source: Source of the memory (manual, hook, batch)
        hygiene_config: Hygiene configuration (optional, uses defaults if None)
        tags: Optional tags for filtering
        artifact_ref: Optional reference to source artifact (session_id, filename, etc.)
        on_commit: Optional hook called with CommitOutcome (M2→M3 bridge)
        event_storage: Optional event storage for M3 event logging
        m4_config: Optional M4 config for entity extraction + graph construction
        pre_extracted_entities: Optional pre-extracted entities (name, type, confidence, evidence)
                                Skips LLM extraction if provided
        temporal_hints: Optional ExtractedMemory with temporal fields (t_valid_hint, temporal_stability_hint)

    Returns:
        CommitOutcome with decision details

    Raises:
        ValueError: If content fails hygiene validation
    """
    from vestig.core.retrieval import cosine_similarity

    # Merge with defaults
    hygiene = {**DEFAULT_HYGIENE, **(hygiene_config or {})}

    # Track thresholds for outcome
    thresholds = {
        "min_chars": hygiene.get("min_chars", DEFAULT_HYGIENE["min_chars"]),
        "max_chars": hygiene.get("max_chars", DEFAULT_HYGIENE["max_chars"]),
    }

    # Basic empty check
    if not content or not content.strip():
        raise ValueError("Memory content cannot be empty")

    # Normalize content
    normalized = normalize_content(
        content, normalize_whitespace=hygiene.get("normalize_whitespace", True)
    )

    # M4: One-shot entity extraction for manual adds (if not pre-extracted)
    # Treat manual content as a "chunk" and extract entities using the same prompt
    extraction_model = None
    extraction_min_confidence = None
    prompt_hash = None
    if pre_extracted_entities is None and m4_config is not None:
        extraction_config = m4_config.get("entity_extraction", {})
        if extraction_config.get("enabled", True):
            from vestig.core.ingestion import extract_memories_from_chunk

            # Get extraction model from M4 config
            extraction_model = extraction_config.get("llm", {}).get("model")
            extraction_min_confidence = extraction_config.get("llm", {}).get(
                "min_confidence", 0.75
            )

            if extraction_model:
                try:
                    if event_storage:
                        from vestig.core.entity_extraction import compute_prompt_hash, load_prompts

                        prompts = load_prompts()
                        template = prompts.get("extract_memories_from_session")
                        if template:
                            prompt_hash = compute_prompt_hash(template)

                    # Extract entities one-shot (treating manual input as a chunk)
                    extracted = extract_memories_from_chunk(
                        normalized,
                        model=extraction_model,
                        min_confidence=extraction_min_confidence,
                    )

                    # Use entities from first extracted memory if available
                    if extracted and len(extracted) > 0:
                        pre_extracted_entities = extracted[0].entities
                    else:
                        pre_extracted_entities = []

                except Exception as e:
                    # Fall back to empty entities if extraction fails
                    pre_extracted_entities = []

    # Calculate content hash early (needed for all outcomes)
    content_hash = hashlib.sha256(normalized.encode("utf-8")).hexdigest()

    # Extract temporal hints if provided
    t_valid_hint = None
    temporal_stability_hint = None
    if temporal_hints is not None:
        # Check if it has temporal fields (duck typing)
        if hasattr(temporal_hints, "t_valid_hint"):
            t_valid_hint = temporal_hints.t_valid_hint
        if hasattr(temporal_hints, "temporal_stability_hint"):
            temporal_stability_hint = temporal_hints.temporal_stability_hint

    # Validate content against hygiene rules
    try:
        validate_content(normalized, hygiene)
    except ValueError as e:
        # Hygiene rejection - build outcome and optionally invoke hook
        outcome = CommitOutcome.rejected_hygiene(
            content_hash=content_hash,
            hygiene_reasons=[str(e)],
            source=source,
            tags=tags,
            artifact_ref=artifact_ref,
            thresholds=thresholds,
        )
        if on_commit:
            on_commit(outcome)
        raise  # Re-raise for backward compatibility

    # Generate embedding
    embedding = embedding_engine.embed_text(normalized)

    # M2: Check for near-duplicates (semantic similarity)
    near_dup_config = hygiene.get("near_duplicate", {})
    near_dup_threshold = near_dup_config.get("threshold", 0.92)
    thresholds["near_duplicate_threshold"] = near_dup_threshold

    matched_id = None
    matched_score = None

    # M3 FIX #6: Performance optimization - skip near-dupe for manual adds
    # Manual CLI adds are intentional, so we trust the user and avoid expensive scan
    skip_manual = near_dup_config.get("skip_manual_source", True)
    should_check_near_dup = near_dup_config.get("enabled", True)

    if should_check_near_dup and (not skip_manual or source != "manual"):
        # TODO(M4): Optimize with time-window (last 30 days) or rolling window (last 1000)
        # For now: full scan for hook/batch/import sources
        all_memories = storage.get_all_memories()

        if all_memories:
            # Find most similar existing memory
            max_score = 0.0
            most_similar_id = None

            for existing in all_memories:
                score = cosine_similarity(embedding, existing.content_embedding)
                if score > max_score:
                    max_score = score
                    most_similar_id = existing.id

            # Check if near-duplicate
            if max_score >= near_dup_threshold:
                # M3 FIX: Don't create new memory - reinforce canonical instead
                matched_id = most_similar_id
                matched_score = max_score

    # M3 FIX: Atomic transaction for store + event + cache updates
    # All DB writes happen in one transaction to prevent partial state
    with storage.conn:
        # M3 FIX (Option A): Near-dupe reinforces canonical, doesn't create new memory
        if matched_id is not None:
            # Near-duplicate detected - reinforce canonical memory, don't insert new
            outcome = CommitOutcome.near_dupe(
                memory_id=matched_id,  # Return canonical ID, not new ID
                matched_memory_id=matched_id,
                query_score=matched_score,
                content_hash=content_hash,
                source=source,
                tags=tags,
                artifact_ref=artifact_ref,
                thresholds=thresholds,
            )
        else:
            # Not a near-duplicate - proceed with normal flow
            # Generate ID
            memory_id = f"mem_{uuid.uuid4()}"

            # Create memory node
            # M3 FIX: Pass pre-computed content_hash to avoid duplication
            node = MemoryNode.create(
                memory_id=memory_id,
                content=normalized,
                embedding=embedding,
                source=source,
                tags=tags,
                content_hash=content_hash,
                t_valid_hint=t_valid_hint,  # Temporal extraction
                temporal_stability_hint=temporal_stability_hint,  # Temporal classification
            )

            # Store (exact dedupe handled in storage layer)
            stored_id = storage.store_memory(node)

            # Build outcome based on what happened
            if stored_id != memory_id:
                # Exact duplicate detected by storage layer (hash match)
                outcome = CommitOutcome.exact_dupe(
                    memory_id=stored_id,
                    content_hash=content_hash,
                    source=source,
                    tags=tags,
                    artifact_ref=artifact_ref,
                    thresholds=thresholds,
                )
            else:
                # New memory inserted
                outcome = CommitOutcome.inserted_new(
                    memory_id=stored_id,
                    content_hash=content_hash,
                    source=source,
                    tags=tags,
                    artifact_ref=artifact_ref,
                    thresholds=thresholds,
                )

        # M4: Entity extraction + MENTIONS edge creation
        # (Only for new memories, not duplicates)
        # (Must happen inside transaction before commit)
        if m4_config and outcome.outcome == "INSERTED_NEW":
            _extract_and_link_entities(
                content=normalized,
                memory_id=outcome.memory_id,
                storage=storage,
                m4_config=m4_config,
                artifact_ref=artifact_ref,
                pre_extracted_entities=pre_extracted_entities,
                event_storage=event_storage,
                extraction_model=extraction_model,
                prompt_hash=prompt_hash,
                min_confidence=extraction_min_confidence,
            )

            # M4: RELATED edge creation (Memory → Memory)
            # Create semantic similarity edges to related memories
            _create_related_edges(
                memory_id=outcome.memory_id,
                embedding=embedding,
                storage=storage,
                m4_config=m4_config,
            )

        # M3: Log event if event_storage provided
        # (Must happen inside transaction before commit)
        if event_storage:
            _log_commit_event(outcome, storage, event_storage)

        # Transaction commits here automatically (context manager)

    # Invoke hook if provided (M2→M3 bridge)
    # (Happens after successful commit)
    if on_commit:
        on_commit(outcome)

    return outcome


def _extract_and_link_entities(
    content: str,
    memory_id: str,
    storage: MemoryStorage,
    m4_config: dict[str, Any],
    artifact_ref: str | None = None,
    pre_extracted_entities: list[tuple[str, str, float, str]] | None = None,
    event_storage: MemoryEventStorage | None = None,
    extraction_model: str | None = None,
    prompt_hash: str | None = None,
    min_confidence: float | None = None,
) -> None:
    """
    Store entities and create MENTIONS edges (M4).

    Called from commit_memory() inside transaction.
    Entities are always pre-extracted using one-shot extraction in commit_memory().

    Args:
        content: Memory content (not used, kept for backward compatibility)
        memory_id: Memory ID (for MENTIONS edges)
        storage: Storage instance
        m4_config: M4 configuration dict
        artifact_ref: Optional artifact reference (not used, kept for backward compatibility)
        pre_extracted_entities: Pre-extracted entities (name, type, confidence, evidence)
    """
    from vestig.core.entity_extraction import store_entities
    from vestig.core.models import EdgeNode

    # Check if entity extraction enabled
    extraction_config = m4_config.get("entity_extraction", {})
    if not extraction_config.get("enabled", True):
        return

    # Store pre-extracted entities (with deduplication)
    # Entities are always extracted one-shot in commit_memory()
    if pre_extracted_entities is None:
        pre_extracted_entities = []

    entities = store_entities(
        entities=pre_extracted_entities,
        memory_id=memory_id,
        storage=storage,
        config=m4_config,
    )

    # Check if MENTIONS edge creation enabled
    mentions_config = m4_config.get("edge_creation", {}).get("mentions", {})
    if not mentions_config.get("enabled", True):
        return

    if event_storage and extraction_model:
        from vestig.core.models import EventNode

        event = EventNode.create(
            memory_id=memory_id,
            event_type="ENTITY_EXTRACTED",
            source="system",
            artifact_ref=artifact_ref,
            payload={
                "model_name": extraction_model,
                "prompt_hash": prompt_hash,
                "entity_count": len(entities),
                "min_confidence": min_confidence,
            },
        )
        event_storage.add_event(event)

    # Create MENTIONS edges for each entity (confidence-gated)
    for entity_id, entity_type, confidence, evidence in entities:
        edge = EdgeNode.create(
            from_node=memory_id,
            to_node=entity_id,
            edge_type="MENTIONS",
            weight=1.0,
            confidence=confidence,
            evidence=evidence,
        )
        storage.store_edge(edge)


def _create_related_edges(
    memory_id: str,
    embedding: list[float],
    storage: MemoryStorage,
    m4_config: dict[str, Any],
) -> None:
    """
    Create RELATED edges to semantically similar memories (M4).

    Called from commit_memory() inside transaction.

    Args:
        memory_id: New memory ID (source of RELATED edges)
        embedding: Embedding vector of new memory
        storage: Storage instance
        m4_config: M4 configuration dict
    """
    from vestig.core.models import EdgeNode
    from vestig.core.retrieval import cosine_similarity

    # Check if RELATED edge creation enabled
    related_config = m4_config.get("edge_creation", {}).get("related", {})
    if not related_config.get("enabled", True):
        return

    # Get config parameters
    similarity_threshold = related_config.get("similarity_threshold", 0.6)
    max_edges_per_memory = related_config.get("max_edges_per_memory", 10)

    # Get all existing memories (exclude current)
    all_memories = storage.get_all_memories()

    # Compute similarity scores
    candidates = []
    for existing in all_memories:
        if existing.id == memory_id:
            continue  # Skip self

        score = cosine_similarity(embedding, existing.content_embedding)

        if score >= similarity_threshold:
            candidates.append((existing.id, score))

    # Sort by score descending and take top-K
    candidates.sort(key=lambda x: x[1], reverse=True)
    top_candidates = candidates[:max_edges_per_memory]

    # Create RELATED edges
    for target_id, score in top_candidates:
        edge = EdgeNode.create(
            from_node=memory_id,
            to_node=target_id,
            edge_type="RELATED",
            weight=score,  # Weight = similarity score
            confidence=score,  # Confidence = similarity score
            evidence=f"semantic_similarity={score:.3f}",
        )
        storage.store_edge(edge)


def _log_commit_event(
    outcome: CommitOutcome,
    storage: MemoryStorage,
    event_storage: MemoryEventStorage,
) -> None:
    """
    Convert CommitOutcome to EventNode and persist (M3).

    Args:
        outcome: The commit outcome to log
        storage: Storage instance for updating convenience fields
        event_storage: Event storage instance
    """
    from vestig.core.models import EventNode

    if outcome.outcome == "REJECTED_HYGIENE":
        return  # Don't log hygiene rejections

    # Map outcome to event type
    event_type_map = {
        "INSERTED_NEW": "ADD",
        "EXACT_DUPE": "REINFORCE_EXACT",
        "NEAR_DUPE": "REINFORCE_NEAR",
    }
    event_type = event_type_map[outcome.outcome]

    # Create event
    event = EventNode.create(
        memory_id=outcome.memory_id,
        event_type=event_type,
        source=outcome.source,
        artifact_ref=outcome.artifact_ref,
        payload={
            "content_hash": outcome.content_hash,
            "tags": outcome.tags,
            "artifact_ref": outcome.artifact_ref,
            "matched_memory_id": outcome.matched_memory_id,
            "query_score": outcome.query_score,
        },
    )
    event_storage.add_event(event)

    # Update convenience fields for reinforcement
    if event_type.startswith("REINFORCE"):
        storage.increment_reinforce_count(outcome.memory_id)
        storage.update_last_seen(outcome.memory_id, outcome.occurred_at)


---
src/vestig/core/config.py
---
"""Configuration loading with minimal error handling"""

from pathlib import Path
from typing import Any

import yaml


def load_config(config_path: str = "config.yaml") -> dict[str, Any]:
    """
    Load configuration from YAML file.

    Args:
        config_path: Path to config file

    Returns:
        Configuration dictionary

    Raises:
        FileNotFoundError: If config file doesn't exist
        yaml.YAMLError: If config file is invalid YAML
    """
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(
            f"Config file not found: {config_path}\n"
            f"Please create a config.yaml file or specify path with --config"
        )

    with open(path) as f:
        config = yaml.safe_load(f)

    # Validate db_path exists in config
    if "storage" not in config or "db_path" not in config["storage"]:
        raise ValueError(f"Invalid config: missing storage.db_path in {config_path}")

    return config


---
src/vestig/core/embeddings.py
---
"""Embedding generation using llm or sentence-transformers"""

import json
import os
import subprocess

# Suppress HuggingFace progress bars
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


class EmbeddingEngine:
    """Wrapper for embedding generation with dimension validation

    Supports two providers:
    - llm: Uses llm CLI (fast, 762ms with ollama)
    - sentence-transformers: Direct model loading (slow, 19s load time)
    """

    def __init__(
        self,
        model_name: str,
        expected_dimension: int,
        normalize: bool = True,
        provider: str = "llm",
        max_length: int | None = None,
    ):
        """
        Initialize embedding engine.

        Args:
            model_name: Model name (format depends on provider)
                - llm: "ollama/nomic-embed-text", "ada-002", etc.
                - sentence-transformers: "BAAI/bge-m3", etc.
            expected_dimension: Expected embedding dimension from config
            normalize: Whether to normalize embeddings for cosine similarity
            provider: "llm" or "sentence-transformers"
            max_length: Maximum character length before truncation (None = no limit)
        """
        self.model_name = model_name
        self.expected_dimension = expected_dimension
        self.normalize = normalize
        self.provider = provider
        self.max_length = max_length

        if provider == "llm":
            # No model loading - llm CLI handles it
            # Validate that llm is available
            try:
                subprocess.run(
                    ["llm", "--version"], capture_output=True, check=True, timeout=5
                )
            except (subprocess.CalledProcessError, FileNotFoundError):
                raise RuntimeError(
                    "llm CLI not found. Install with: pip install llm\n"
                    "See: https://llm.datasette.io/"
                )

            # Validate dimension with a test embedding
            test_embedding = self._embed_with_llm("test")
            if len(test_embedding) != expected_dimension:
                raise ValueError(
                    f"Embedding dimension mismatch: expected {expected_dimension}, "
                    f"got {len(test_embedding)} for model {model_name}"
                )

        elif provider == "sentence-transformers":
            # Legacy: Load model directly (slow)
            from sentence_transformers import SentenceTransformer

            # Some models (e.g., BAAI/bge-m3) do not ship safetensors
            self.model = SentenceTransformer(
                model_name, model_kwargs={"use_safetensors": False}
            )

            # Validate dimension on initialization
            test_embedding = self.model.encode("test", normalize_embeddings=self.normalize)
            actual_dimension = len(test_embedding)
            if actual_dimension != expected_dimension:
                raise ValueError(
                    f"Embedding dimension mismatch: expected {expected_dimension}, "
                    f"got {actual_dimension}"
                )
        else:
            raise ValueError(f"Unknown provider: {provider}. Use 'llm' or 'sentence-transformers'")

    def _truncate_text(self, text: str) -> str:
        """Truncate text to max_length if configured"""
        if self.max_length and len(text) > self.max_length:
            return text[: self.max_length]
        return text

    def _embed_with_llm(self, text: str) -> list[float]:
        """
        Generate embedding using llm CLI.

        Args:
            text: Input text to embed

        Returns:
            Embedding vector as list of floats
        """
        text = self._truncate_text(text)
        try:
            result = subprocess.run(
                ["llm", "embed", "-c", text, "-m", self.model_name, "-f", "json"],
                capture_output=True,
                text=True,
                check=True,
                timeout=30,
            )
            embedding = json.loads(result.stdout)
            return embedding
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"llm embed failed: {e.stderr}")
        except json.JSONDecodeError as e:
            raise RuntimeError(f"Failed to parse llm output: {e}")

    def embed_text(self, text: str) -> list[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Input text to embed

        Returns:
            Normalized embedding vector as list of floats
        """
        if self.provider == "llm":
            return self._embed_with_llm(text)
        else:  # sentence-transformers
            text = self._truncate_text(text)
            embedding = self.model.encode(
                text, normalize_embeddings=self.normalize, show_progress_bar=False
            )
            return embedding.tolist()

    def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of input texts to embed

        Returns:
            List of normalized embedding vectors
        """
        if self.provider == "llm":
            # llm doesn't have native batch support, so process one by one
            # Could optimize with parallel processing if needed
            return [self._embed_with_llm(text) for text in texts]
        else:  # sentence-transformers
            embeddings = self.model.encode(
                texts, normalize_embeddings=self.normalize, show_progress_bar=False
            )
            return embeddings.tolist()


---
src/vestig/core/entity_extraction.py
---
"""Entity extraction using LLM (M4: Graph Layer)"""

import hashlib
import json
from pathlib import Path
from typing import Any

import yaml
from pydantic import BaseModel


def load_prompts(prompts_path: str | None = None) -> dict[str, Any]:
    """
    Load prompts from YAML file.

    Args:
        prompts_path: Optional path to prompts.yaml. If None, uses prompts.yaml
                     in the same directory as this module.

    Returns:
        Dictionary of prompt templates (supports both string and dict formats)
        - String format (legacy): prompt_name: "prompt text"
        - Dict format (M4+): prompt_name: {system: "...", user: "...", description: "..."}
    """
    if prompts_path is None:
        # Default: prompts.yaml in same directory as this module
        module_dir = Path(__file__).parent
        path = module_dir / "prompts.yaml"
    else:
        path = Path(prompts_path)

    if not path.exists():
        raise FileNotFoundError(f"Prompts file not found: {path}")

    with open(path) as f:
        prompts = yaml.safe_load(f)

    return prompts


def substitute_tokens(template: str | dict[str, str], **kwargs) -> str | dict[str, str]:
    """
    Substitute {{token}} placeholders in template.

    Args:
        template: Template string or dict with 'system'/'user' keys
        **kwargs: Token values

    Returns:
        Template with tokens substituted (same type as input)

    Examples:
        >>> substitute_tokens("Hello {{name}}", name="Alice")
        'Hello Alice'
        >>> substitute_tokens({"system": "You are {{role}}", "user": "Do {{task}}"}, role="helper", task="summarize")
        {'system': 'You are helper', 'user': 'Do summarize'}
    """
    if isinstance(template, dict):
        # M4+: Handle dict format with system/user prompts
        result = {}
        if "system" in template:
            result["system"] = template["system"]
            for key, value in kwargs.items():
                placeholder = f"{{{{{key}}}}}"
                result["system"] = result["system"].replace(placeholder, str(value))
        if "user" in template:
            result["user"] = template["user"]
            for key, value in kwargs.items():
                placeholder = f"{{{{{key}}}}}"
                result["user"] = result["user"].replace(placeholder, str(value))
        return result
    else:
        # Legacy: Handle string format
        result = template
        for key, value in kwargs.items():
            placeholder = f"{{{{{key}}}}}"
            result = result.replace(placeholder, str(value))
        return result


def call_llm(
    prompt: str | dict[str, str],
    model: str,
    schema: type[BaseModel] | None = None
):
    """
    Call LLM with prompt using llm module.

    Args:
        prompt: Full prompt text (string) or dict with 'system'/'user' keys (M4+)
        model: Model name (e.g., "claude-haiku-4.5")
        schema: Optional Pydantic model for structured output

    Returns:
        If schema provided: Pydantic model instance
        If no schema: str (response text)

    Raises:
        ImportError: If llm module not installed
        Exception: If LLM call fails
    """
    try:
        import llm
    except ImportError:
        raise ImportError("llm module not installed. Install with: pip install llm")

    try:
        # Get model and call it
        llm_model = llm.get_model(model)

        # Handle both string and dict formats
        if isinstance(prompt, dict):
            # M4+: Dict format with system/user prompts
            system_text = prompt.get("system", "")
            user_text = prompt.get("user", "")

            if schema:
                # Use structured output with schema
                response = llm_model.prompt(user_text, system=system_text, schema=schema)
                # Parse JSON response and validate with schema
                json_text = response.text()
                data = json.loads(json_text)
                return schema.model_validate(data)
            else:
                # Return raw text
                response = llm_model.prompt(user_text, system=system_text)
                return response.text()
        else:
            # Legacy: String format (no system prompt)
            if schema:
                # Use structured output with schema
                response = llm_model.prompt(prompt, schema=schema)
                # Parse JSON response and validate with schema
                json_text = response.text()
                data = json.loads(json_text)
                return schema.model_validate(data)
            else:
                # Return raw text
                response = llm_model.prompt(prompt)
                return response.text()

    except Exception as e:
        raise Exception(f"LLM call failed: {e}")


def compute_prompt_hash(template: str) -> str:
    """Compute stable hash for prompt template tracking."""
    return hashlib.sha256(template.encode("utf-8")).hexdigest()[:16]


def store_entities(
    entities: list[tuple[str, str, float, str]],
    memory_id: str,
    storage,  # MemoryStorage instance
    config: dict[str, Any],
) -> list[tuple]:
    """
    Store pre-extracted entities with deduplication.

    Used when entities are already extracted (e.g., combined extraction during ingestion).

    Args:
        entities: List of (name, type, confidence, evidence) tuples
        memory_id: Memory ID (for event logging)
        storage: MemoryStorage instance
        config: M4 config dict

    Returns:
        List of (entity_id, entity_type, confidence, evidence) tuples
        for entities that passed confidence threshold
    """
    from vestig.core.models import EntityNode, compute_norm_key

    # Get config
    min_confidence = config.get("entity_extraction", {}).get("llm", {}).get("min_confidence", 0.75)

    # Store entities with deduplication
    stored_entities = []

    for name, entity_type, confidence, evidence in entities:
        # Apply confidence threshold
        if confidence < min_confidence:
            continue

        # Compute norm_key for deduplication
        norm_key = compute_norm_key(name, entity_type)

        # Find or create entity
        existing = storage.find_entity_by_norm_key(norm_key, include_expired=False)

        if existing:
            # Entity already exists - use existing ID
            entity_id = existing.id
        else:
            # Create new entity
            new_entity = EntityNode.create(
                entity_type=entity_type,
                canonical_name=name,
            )
            entity_id = storage.store_entity(new_entity)

        # Return entity info for edge creation
        stored_entities.append((entity_id, entity_type, confidence, evidence))

    return stored_entities


---
src/vestig/core/event_storage.py
---
"""Event storage for M3 lifecycle tracking"""

import json
import sqlite3

from vestig.core.models import EventNode


class MemoryEventStorage:
    """Event CRUD operations (shares DB connection with MemoryStorage)"""

    def __init__(self, conn: sqlite3.Connection):
        """
        Use same DB connection as MemoryStorage for transaction consistency.

        Args:
            conn: SQLite connection from MemoryStorage
        """
        self.conn = conn

    def add_event(self, event: EventNode) -> str:
        """
        Insert event (append-only, never update).

        Args:
            event: EventNode to insert

        Returns:
            Event ID
        """
        self.conn.execute(
            """
            INSERT INTO memory_events
            (event_id, memory_id, event_type, occurred_at, source, actor, artifact_ref, payload_json)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                event.event_id,
                event.memory_id,
                event.event_type,
                event.occurred_at,
                event.source,
                event.actor,
                event.artifact_ref,
                json.dumps(event.payload),
            ),
        )
        # NOTE: Caller manages transaction commit
        return event.event_id

    def get_events_for_memory(self, memory_id: str, limit: int = 100) -> list[EventNode]:
        """
        Retrieve events for a memory, newest first.

        Args:
            memory_id: Memory ID to retrieve events for
            limit: Maximum number of events to return

        Returns:
            List of EventNode objects
        """
        cursor = self.conn.execute(
            """
            SELECT event_id, memory_id, event_type, occurred_at, source, actor, artifact_ref, payload_json
            FROM memory_events
            WHERE memory_id = ?
            ORDER BY occurred_at DESC
            LIMIT ?
            """,
            (memory_id, limit),
        )
        return [
            EventNode(
                event_id=row[0],
                memory_id=row[1],
                event_type=row[2],
                occurred_at=row[3],
                source=row[4],
                actor=row[5],
                artifact_ref=row[6],
                payload=json.loads(row[7]),
            )
            for row in cursor.fetchall()
        ]

    def get_reinforcement_events(self, memory_id: str) -> list[EventNode]:
        """
        Get only REINFORCE_* events for TraceRank computation.

        Args:
            memory_id: Memory ID to retrieve reinforcement events for

        Returns:
            List of EventNode objects with event_type like 'REINFORCE_%'
        """
        cursor = self.conn.execute(
            """
            SELECT event_id, memory_id, event_type, occurred_at, source, actor, artifact_ref, payload_json
            FROM memory_events
            WHERE memory_id = ? AND event_type LIKE 'REINFORCE_%'
            ORDER BY occurred_at DESC
            """,
            (memory_id,),
        )
        return [
            EventNode(
                event_id=row[0],
                memory_id=row[1],
                event_type=row[2],
                occurred_at=row[3],
                source=row[4],
                actor=row[5],
                artifact_ref=row[6],
                payload=json.loads(row[7]),
            )
            for row in cursor.fetchall()
        ]


---
src/vestig/core/events.py
---
"""Event and outcome types for M2→M3 bridge"""

from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any


@dataclass
class CommitOutcome:
    """
    Structured outcome of a commit attempt (M2→M3 bridge).

    This captures the decision made during commit_memory() in a way that
    can be consumed by M3 event logging and TraceRank mechanics.

    Outcome states:
    - INSERTED_NEW: A new Memory Node was created
    - EXACT_DUPE: Hash match to existing memory (same ID returned)
    - NEAR_DUPE: Semantic match above threshold to existing memory
    - REJECTED_HYGIENE: Blocked by hygiene rule(s)
    """

    outcome: str  # INSERTED_NEW | EXACT_DUPE | NEAR_DUPE | REJECTED_HYGIENE
    memory_id: str  # The canonical ID (existing or new)
    content_hash: str  # SHA256 of normalized content
    occurred_at: str  # UTC timestamp (ISO 8601)
    source: str = "manual"  # manual | hook | import | batch

    # Dupe/near-dupe fields
    matched_memory_id: str | None = None  # ID of matched memory (if dupe)
    query_score: float | None = None  # Similarity score (for near-dupe)

    # Hygiene rejection fields
    hygiene_reasons: list[str] = field(default_factory=list)  # Rejection reasons

    # Metadata
    thresholds: dict[str, Any] = field(default_factory=dict)  # Config thresholds used
    tags: list[str] | None = None
    artifact_ref: str | None = None  # Session ID, filename, URL, etc.

    @classmethod
    def inserted_new(
        cls,
        memory_id: str,
        content_hash: str,
        source: str = "manual",
        tags: list[str] | None = None,
        artifact_ref: str | None = None,
        thresholds: dict[str, Any] | None = None,
    ) -> "CommitOutcome":
        """Create outcome for a new memory insertion."""
        return cls(
            outcome="INSERTED_NEW",
            memory_id=memory_id,
            content_hash=content_hash,
            occurred_at=datetime.now(timezone.utc).isoformat(),
            source=source,
            tags=tags,
            artifact_ref=artifact_ref,
            thresholds=thresholds or {},
        )

    @classmethod
    def exact_dupe(
        cls,
        memory_id: str,
        content_hash: str,
        source: str = "manual",
        tags: list[str] | None = None,
        artifact_ref: str | None = None,
        thresholds: dict[str, Any] | None = None,
    ) -> "CommitOutcome":
        """Create outcome for an exact duplicate (hash match)."""
        return cls(
            outcome="EXACT_DUPE",
            memory_id=memory_id,
            content_hash=content_hash,
            occurred_at=datetime.now(timezone.utc).isoformat(),
            source=source,
            matched_memory_id=memory_id,
            tags=tags,
            artifact_ref=artifact_ref,
            thresholds=thresholds or {},
        )

    @classmethod
    def near_dupe(
        cls,
        memory_id: str,
        matched_memory_id: str,
        query_score: float,
        content_hash: str,
        source: str = "manual",
        tags: list[str] | None = None,
        artifact_ref: str | None = None,
        thresholds: dict[str, Any] | None = None,
    ) -> "CommitOutcome":
        """Create outcome for a near-duplicate (semantic match)."""
        return cls(
            outcome="NEAR_DUPE",
            memory_id=memory_id,
            content_hash=content_hash,
            occurred_at=datetime.now(timezone.utc).isoformat(),
            source=source,
            matched_memory_id=matched_memory_id,
            query_score=query_score,
            tags=tags,
            artifact_ref=artifact_ref,
            thresholds=thresholds or {},
        )

    @classmethod
    def rejected_hygiene(
        cls,
        content_hash: str,
        hygiene_reasons: list[str],
        source: str = "manual",
        tags: list[str] | None = None,
        artifact_ref: str | None = None,
        thresholds: dict[str, Any] | None = None,
    ) -> "CommitOutcome":
        """Create outcome for a hygiene rejection."""
        return cls(
            outcome="REJECTED_HYGIENE",
            memory_id="",  # No ID assigned
            content_hash=content_hash,
            occurred_at=datetime.now(timezone.utc).isoformat(),
            source=source,
            hygiene_reasons=hygiene_reasons,
            tags=tags,
            artifact_ref=artifact_ref,
            thresholds=thresholds or {},
        )


# Hook type for M3 event logging
OnCommitHook = Callable[[CommitOutcome], None]


---
src/vestig/core/graph.py
---
"""Graph traversal and expansion (M4: Graph Layer)"""

from typing import Any

from vestig.core.storage import MemoryStorage


def expand_via_entities(
    memory_ids: list[str],
    storage: MemoryStorage,
    limit: int = 5,
    include_expired: bool = False,
    min_confidence: float = 0.75,
) -> list[dict[str, Any]]:
    """
    Expand via shared entities (1-hop through MENTIONS edges).

    Given a list of memory IDs, find memories that share entities with them.

    Args:
        memory_ids: List of source memory IDs
        storage: Storage instance
        limit: Maximum number of expanded memories to return
        include_expired: Include expired memories/edges
        min_confidence: Minimum confidence for MENTIONS edges

    Returns:
        List of dicts with:
            - memory: MemoryNode object
            - retrieval_reason: "graph_expansion_entity"
            - shared_entities: List of shared entity IDs
            - expansion_score: Number of shared entities (for ranking)
    """
    # Track candidate memories and their shared entities
    candidates: dict[str, set[str]] = {}  # memory_id -> set of shared entity IDs

    # For each source memory, find connected entities
    for source_id in memory_ids:
        # Get MENTIONS edges from source memory
        mentions_edges = storage.get_edges_from_memory(
            source_id,
            edge_type="MENTIONS",
            include_expired=include_expired,
            min_confidence=min_confidence,
        )

        # For each entity, find other memories that mention it
        for edge in mentions_edges:
            entity_id = edge.to_node

            # Find all memories that mention this entity (incoming edges to entity)
            incoming_edges = storage.get_edges_to_entity(
                entity_id,
                include_expired=include_expired,
                min_confidence=min_confidence,
            )

            # Add candidates (exclude source memories)
            for incoming_edge in incoming_edges:
                candidate_id = incoming_edge.from_node

                # Skip if same as source or already in source list
                if candidate_id == source_id or candidate_id in memory_ids:
                    continue

                # Track shared entity
                if candidate_id not in candidates:
                    candidates[candidate_id] = set()
                candidates[candidate_id].add(entity_id)

    # Convert to list and score by number of shared entities
    results = []
    for memory_id, shared_entities in candidates.items():
        memory = storage.get_memory(memory_id)

        if memory is None:
            continue  # Memory deleted or not found

        # Skip expired memories (unless include_expired=True)
        if not include_expired and memory.t_expired is not None:
            continue

        results.append(
            {
                "memory": memory,
                "retrieval_reason": "graph_expansion_entity",
                "shared_entities": list(shared_entities),
                "expansion_score": len(shared_entities),
            }
        )

    # Sort by expansion score descending (more shared entities = higher score)
    results.sort(key=lambda x: x["expansion_score"], reverse=True)

    # Return top-K
    return results[:limit]


def expand_via_related(
    memory_ids: list[str],
    storage: MemoryStorage,
    limit: int = 5,
    include_expired: bool = False,
    min_confidence: float = 0.0,
) -> list[dict[str, Any]]:
    """
    Expand via RELATED edges (1-hop semantic similarity).

    Given a list of memory IDs, find memories connected via RELATED edges.

    Args:
        memory_ids: List of source memory IDs
        storage: Storage instance
        limit: Maximum number of expanded memories to return
        include_expired: Include expired memories/edges
        min_confidence: Minimum confidence for RELATED edges

    Returns:
        List of dicts with:
            - memory: MemoryNode object
            - retrieval_reason: "graph_expansion_related"
            - similarity_score: Edge weight (semantic similarity)
            - source_memory_id: Which source memory connected to this
    """
    # Track candidate memories and their best similarity score
    candidates: dict[str, tuple] = {}  # memory_id -> (score, source_id)

    # For each source memory, find RELATED edges
    for source_id in memory_ids:
        # Get RELATED edges from source memory
        related_edges = storage.get_edges_from_memory(
            source_id,
            edge_type="RELATED",
            include_expired=include_expired,
            min_confidence=min_confidence,
        )

        # Add candidates (exclude source memories)
        for edge in related_edges:
            candidate_id = edge.to_node

            # Skip if same as source or already in source list
            if candidate_id == source_id or candidate_id in memory_ids:
                continue

            # Track best (highest) similarity score for this candidate
            score = edge.weight
            if candidate_id not in candidates or score > candidates[candidate_id][0]:
                candidates[candidate_id] = (score, source_id)

    # Convert to list
    results = []
    for memory_id, (score, source_id) in candidates.items():
        memory = storage.get_memory(memory_id)

        if memory is None:
            continue  # Memory deleted or not found

        # Skip expired memories (unless include_expired=True)
        if not include_expired and memory.t_expired is not None:
            continue

        results.append(
            {
                "memory": memory,
                "retrieval_reason": "graph_expansion_related",
                "similarity_score": score,
                "source_memory_id": source_id,
            }
        )

    # Sort by similarity score descending (higher similarity = better)
    results.sort(key=lambda x: x["similarity_score"], reverse=True)

    # Return top-K
    return results[:limit]


def expand_with_graph(
    memory_ids: list[str],
    storage: MemoryStorage,
    entity_limit: int = 3,
    related_limit: int = 3,
    include_expired: bool = False,
) -> dict[str, list[dict[str, Any]]]:
    """
    Perform both entity and related expansions in one call.

    Convenience function that combines entity and related expansion.

    Args:
        memory_ids: List of source memory IDs
        storage: Storage instance
        entity_limit: Max memories to return from entity expansion
        related_limit: Max memories to return from related expansion
        include_expired: Include expired memories/edges

    Returns:
        Dict with two keys:
            - "via_entities": Results from expand_via_entities
            - "via_related": Results from expand_via_related
    """
    entity_expansion = expand_via_entities(
        memory_ids=memory_ids,
        storage=storage,
        limit=entity_limit,
        include_expired=include_expired,
    )

    related_expansion = expand_via_related(
        memory_ids=memory_ids,
        storage=storage,
        limit=related_limit,
        include_expired=include_expired,
    )

    return {
        "via_entities": entity_expansion,
        "via_related": related_expansion,
    }


---
src/vestig/core/ingest_sources.py
---
"""Normalization helpers for ingesting different document formats."""

from __future__ import annotations

import json
from datetime import datetime, timezone
from pathlib import Path
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from vestig.core.ingestion import TemporalHints

SUPPORTED_FORMATS = {"auto", "plain", "claude-session"}


def detect_format(path: Path | None, raw_text: str) -> str:
    if path and path.suffix.lower() == ".jsonl":
        if _looks_like_claude_session(raw_text):
            return "claude-session"
    return "plain"


def normalize_document_text(
    raw_text: str,
    source_format: str = "auto",
    format_config: dict[str, Any] | None = None,
    path: Path | None = None,
) -> tuple[str, str, "TemporalHints"]:
    """
    Normalize document text and extract temporal hints.

    Returns:
        Tuple of (normalized_text, resolved_format, temporal_hints)
    """
    # Import here to avoid circular dependency
    from vestig.core.ingestion import TemporalHints

    if source_format not in SUPPORTED_FORMATS:
        raise ValueError(f"Unknown ingest format: {source_format}")

    resolved_format = source_format
    if source_format == "auto":
        resolved_format = detect_format(path, raw_text)

    if resolved_format == "plain":
        # Plain text: Use file mtime if available, else now
        if path:
            temporal_hints = TemporalHints.from_file_mtime(path)
        else:
            temporal_hints = TemporalHints.from_now()
        return raw_text, resolved_format, temporal_hints

    if resolved_format == "claude-session":
        session_config = format_config or {}
        normalized_text, temporal_hints = extract_claude_session_text_with_temporal(
            raw_text, session_config
        )
        return normalized_text, resolved_format, temporal_hints

    raise ValueError(f"Unhandled ingest format: {resolved_format}")


def _looks_like_claude_session(raw_text: str) -> bool:
    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            event = json.loads(line)
        except json.JSONDecodeError:
            return False
        if not isinstance(event, dict):
            return False
        event_type = event.get("type")
        if event_type in {"user", "assistant"}:
            return True
        message = event.get("message")
        if isinstance(message, dict) and message.get("role") in {"user", "assistant"}:
            return True
    return False


def _normalize_timestamp(timestamp: str | None) -> str | None:
    if not timestamp or not isinstance(timestamp, str):
        return None
    try:
        normalized = timestamp.replace("Z", "+00:00")
        dt = datetime.fromisoformat(normalized)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        else:
            dt = dt.astimezone(timezone.utc)
        return dt.isoformat()
    except ValueError:
        return None


def _earliest_timestamp(timestamps: list[str]) -> str | None:
    if not timestamps:
        return None
    parsed = []
    for ts in timestamps:
        try:
            parsed.append(datetime.fromisoformat(ts))
        except ValueError:
            continue
    if not parsed:
        return None
    return min(parsed).isoformat()


def _extract_text_from_block(block: Any) -> str:
    if isinstance(block, str):
        return block.strip()
    if isinstance(block, dict):
        text = block.get("text", "")
        if isinstance(text, str):
            return text.strip()
    return ""


def extract_claude_session_text(raw_text: str, config: dict[str, Any]) -> str:
    """Extract text from claude-session JSONL (backward compatible, no temporal)."""
    include_roles = set(config.get("include_roles", ["user", "assistant"]))
    include_message_types = set(config.get("include_message_types", ["text"]))
    drop_thinking = config.get("drop_thinking", True)
    drop_tool_use = config.get("drop_tool_use", True)

    blocks: list[str] = []
    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            event = json.loads(line)
        except json.JSONDecodeError:
            continue
        if not isinstance(event, dict):
            continue

        event_type = event.get("type")
        message = event.get("message") if isinstance(event, dict) else None
        role = None
        content = None

        if isinstance(message, dict):
            role = message.get("role")
            content = message.get("content")
        elif event_type in {"user", "assistant"}:
            role = event_type
            content = event.get("content")

        if role not in include_roles:
            continue

        filtered_blocks = _filter_message_blocks(
            role,
            content,
            include_message_types=include_message_types,
            drop_thinking=drop_thinking,
            drop_tool_use=drop_tool_use,
        )
        blocks.extend(filtered_blocks)

    return "\n\n".join(blocks)


def extract_claude_session_text_with_temporal(
    raw_text: str,
    config: dict[str, Any]
) -> tuple[str, "TemporalHints"]:
    """
    Extract text from claude-session JSONL with temporal metadata.

    Returns:
        Tuple of (normalized_text, temporal_hints)
    """
    from vestig.core.ingestion import TemporalHints

    include_roles = set(config.get("include_roles", ["user", "assistant"]))
    include_message_types = set(config.get("include_message_types", ["text"]))
    drop_thinking = config.get("drop_thinking", True)
    drop_tool_use = config.get("drop_tool_use", True)

    blocks: list[str] = []
    timestamps: list[str] = []  # Collect all event timestamps

    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            event = json.loads(line)
        except json.JSONDecodeError:
            continue
        if not isinstance(event, dict):
            continue

        # Extract timestamp from event (if present)
        event_timestamp = _normalize_timestamp(event.get("timestamp"))
        if event_timestamp:
            timestamps.append(event_timestamp)

        event_type = event.get("type")
        message = event.get("message") if isinstance(event, dict) else None
        role = None
        content = None

        if isinstance(message, dict):
            role = message.get("role")
            content = message.get("content")
        elif event_type in {"user", "assistant"}:
            role = event_type
            content = event.get("content")

        if role not in include_roles:
            continue

        filtered_blocks = _filter_message_blocks(
            role,
            content,
            include_message_types=include_message_types,
            drop_thinking=drop_thinking,
            drop_tool_use=drop_tool_use,
        )
        blocks.extend(filtered_blocks)

    normalized_text = "\n\n".join(blocks)

    # Extract temporal hints from earliest timestamp
    if timestamps:
        earliest = _earliest_timestamp(timestamps) or timestamps[0]
        temporal_hints = TemporalHints.from_timestamp(
            timestamp=earliest,
            evidence=f"Extracted from claude-session JSONL (earliest of {len(timestamps)} events)"
        )
    else:
        # No timestamps found - fallback to now
        temporal_hints = TemporalHints.from_now()

    return normalized_text, temporal_hints


def extract_claude_session_chunks(
    raw_text: str,
    config: dict[str, Any],
    chunk_size: int,
    chunk_overlap: int,
) -> list[tuple[str, "TemporalHints"]]:
    """
    Extract chunks from claude-session JSONL with per-chunk temporal hints.
    """
    from vestig.core.ingestion import TemporalHints

    include_roles = set(config.get("include_roles", ["user", "assistant"]))
    include_message_types = set(config.get("include_message_types", ["text"]))
    drop_thinking = config.get("drop_thinking", True)
    drop_tool_use = config.get("drop_tool_use", True)

    blocks: list[tuple[str, str | None]] = []

    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            event = json.loads(line)
        except json.JSONDecodeError:
            continue
        if not isinstance(event, dict):
            continue

        event_timestamp = _normalize_timestamp(event.get("timestamp"))

        event_type = event.get("type")
        message = event.get("message") if isinstance(event, dict) else None
        role = None
        content = None

        if isinstance(message, dict):
            role = message.get("role")
            content = message.get("content")
        elif event_type in {"user", "assistant"}:
            role = event_type
            content = event.get("content")

        if role not in include_roles:
            continue

        filtered_blocks = _filter_message_blocks(
            role,
            content,
            include_message_types=include_message_types,
            drop_thinking=drop_thinking,
            drop_tool_use=drop_tool_use,
        )
        for block in filtered_blocks:
            blocks.append((block, event_timestamp))

    chunked = _chunk_blocks(blocks, chunk_size, chunk_overlap)
    chunks_with_hints: list[tuple[str, TemporalHints]] = []
    for chunk_text, timestamps in chunked:
        earliest = _earliest_timestamp(timestamps)
        if earliest:
            hints = TemporalHints.from_timestamp(
                timestamp=earliest,
                evidence=f"Extracted from claude-session JSONL chunk ({len(timestamps)} events)"
            )
        else:
            hints = TemporalHints.from_now()
        chunks_with_hints.append((chunk_text, hints))

    return chunks_with_hints


def _chunk_blocks(
    blocks: list[tuple[str, str | None]],
    chunk_size: int,
    chunk_overlap: int,
) -> list[tuple[str, list[str]]]:
    sep = "\n\n"
    sep_len = len(sep)
    chunks: list[tuple[str, list[str]]] = []
    current: list[tuple[str, str | None]] = []
    current_len = 0

    def packed_length(parts: list[tuple[str, str | None]]) -> int:
        if not parts:
            return 0
        total = sum(len(text) for text, _ in parts)
        total += sep_len * (len(parts) - 1)
        return total

    def pack(parts: list[tuple[str, str | None]]) -> tuple[str, list[str]]:
        text = sep.join(text for text, _ in parts)
        timestamps = [ts for _, ts in parts if ts]
        return text, timestamps

    def overlap_blocks(parts: list[tuple[str, str | None]]) -> list[tuple[str, str | None]]:
        if chunk_overlap <= 0:
            return []
        overlap_parts: list[tuple[str, str | None]] = []
        overlap_len = 0
        for text, ts in reversed(parts):
            add_len = len(text) + (sep_len if overlap_parts else 0)
            overlap_len += add_len
            overlap_parts.append((text, ts))
            if overlap_len >= chunk_overlap:
                break
        overlap_parts.reverse()
        return overlap_parts

    for text, ts in blocks:
        add_len = len(text) + (sep_len if current else 0)
        if current and current_len + add_len > chunk_size:
            chunks.append(pack(current))
            current = overlap_blocks(current)
            current_len = packed_length(current)

        if not current and len(text) > chunk_size:
            chunks.append((text, [ts] if ts else []))
            continue

        add_len = len(text) + (sep_len if current else 0)
        current.append((text, ts))
        current_len += add_len

    if current:
        chunks.append(pack(current))

    return chunks


def _filter_message_blocks(
    role: str,
    content: Any,
    *,
    include_message_types: set[str],
    drop_thinking: bool,
    drop_tool_use: bool,
) -> list[str]:
    if isinstance(content, str):
        return [f"{role}: {content.strip()}"] if content.strip() else []

    items: list[Any]
    if isinstance(content, list):
        items = content
    elif isinstance(content, dict):
        items = [content]
    else:
        return []

    blocks: list[str] = []
    for item in items:
        if isinstance(item, dict):
            item_type = item.get("type")
            if item_type == "thinking" and drop_thinking:
                continue
            if item_type in {"tool_use", "tool_result"} and drop_tool_use:
                continue
            if item_type and item_type not in include_message_types:
                continue
        text = _extract_text_from_block(item)
        if text:
            blocks.append(f"{role}: {text}")

    return blocks


---
src/vestig/core/ingestion.py
---
"""Document ingestion with LLM-based memory extraction"""

from dataclasses import dataclass
from pathlib import Path
from typing import Any

from pydantic import BaseModel, Field

from vestig.core.commitment import commit_memory
from vestig.core.embeddings import EmbeddingEngine
from vestig.core.entity_extraction import (
    call_llm,
    load_prompts,
    substitute_tokens,
)
from vestig.core.event_storage import MemoryEventStorage
from vestig.core.ingest_sources import (
    extract_claude_session_chunks,
    normalize_document_text,
)
from vestig.core.models import MemoryNode
from vestig.core.storage import MemoryStorage


@dataclass
class TemporalHints:
    """
    Temporal metadata extracted by parsers.

    Represents the temporal context for a document/chunk being ingested.
    These hints flow from parser → ExtractedMemory → MemoryNode.
    """

    # Primary timestamp for this content
    t_valid: str | None = None  # ISO 8601 timestamp

    # Temporal classification
    stability: str = "unknown"  # "static" | "dynamic" | "ephemeral" | "unknown"

    # Evidence for debugging
    extraction_method: str = (
        "default"  # "jsonl_timestamp" | "file_mtime" | "filename_pattern" | "default"
    )
    evidence: str | None = None  # Human-readable explanation

    @classmethod
    def from_now(cls) -> "TemporalHints":
        """Create hints with current time (fallback)."""
        from datetime import datetime, timezone

        now = datetime.now(timezone.utc).isoformat()
        return cls(
            t_valid=now,
            stability="unknown",
            extraction_method="default",
            evidence="No temporal metadata available; using current time",
        )

    @classmethod
    def from_file_mtime(cls, path: Path) -> "TemporalHints":
        """Extract temporal hints from file modification time."""
        from datetime import datetime, timezone

        mtime = path.stat().st_mtime
        dt = datetime.fromtimestamp(mtime, tz=timezone.utc)
        return cls(
            t_valid=dt.isoformat(),
            stability="unknown",
            extraction_method="file_mtime",
            evidence=f"Extracted from file modification time: {path.name}",
        )

    @classmethod
    def from_timestamp(cls, timestamp: str, evidence: str) -> "TemporalHints":
        """Create hints from explicit timestamp (e.g., JSONL event)."""
        return cls(
            t_valid=timestamp,
            stability="unknown",
            extraction_method="jsonl_timestamp",
            evidence=evidence,
        )


@dataclass
class ExtractedMemory:
    """Memory extracted from document by LLM with temporal hints"""

    content: str
    confidence: float
    rationale: str
    entities: list[tuple[str, str, float, str]]  # (name, type, confidence, evidence)

    # Temporal hints (optional, None = use defaults)
    t_valid_hint: str | None = None  # When fact became true (ISO 8601)
    temporal_stability_hint: str | None = None  # "static" | "dynamic" | "unknown"
    temporal_evidence: str | None = None  # Brief explanation of temporal extraction


# Pydantic schemas for LLM structured output
class EntitySchema(BaseModel):
    """Schema for an extracted entity"""

    name: str = Field(description="Entity name or identifier")
    type: str = Field(
        description="Entity type: PERSON, ORG, SYSTEM, PROJECT, PLACE, SKILL, TOOL, FILE, or CONCEPT"
    )
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence score 0.0-1.0")
    evidence: str = Field(description="Text snippet from memory that supports this entity")


class MemorySchema(BaseModel):
    """Schema for a single memory with entities"""

    content: str = Field(
        description="The full memory text with enough context to be self-contained"
    )
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence score 0.0-1.0")
    rationale: str = Field(description="Brief explanation of why this is worth remembering")
    temporal_stability: str = Field(
        default="unknown",
        description="Temporal stability: static (permanent), dynamic (changing), ephemeral (expected to change soon), or unknown",
    )
    entities: list[EntitySchema] = Field(
        default_factory=list,
        description="Entities mentioned in this memory (people, orgs, systems, projects, places)",
    )


class MemoryExtractionResult(BaseModel):
    """Schema for memory extraction response"""

    memories: list[MemorySchema] = Field(description="List of extracted memories")


class SummaryBullet(BaseModel):
    """A single bullet point in a summary with citations"""

    text: str = Field(description="The bullet point text")
    memory_ids: list[str] = Field(description="Memory IDs supporting this bullet (1-4 IDs)")


class SummaryData(BaseModel):
    """The summary content"""

    title: str = Field(description="Short descriptive title")
    scope: str = Field(description="Summary scope - typically INGEST_RUN")
    overview: str = Field(description="2-4 sentence overview in plain language")
    bullets: list[SummaryBullet] = Field(description="6-12 key insights with citations")
    themes: list[str] = Field(description="3-8 short theme tags")
    open_questions: list[str] = Field(
        default_factory=list, description="Genuine gaps or ambiguities"
    )


class SummaryResult(BaseModel):
    """Schema for summary generation response (M4)"""

    summary: SummaryData = Field(description="The generated summary")


@dataclass
class IngestionResult:
    """Result of document ingestion"""

    document_path: str
    chunks_processed: int
    memories_extracted: int
    memories_committed: int
    memories_deduplicated: int
    entities_created: int
    errors: list[str]


def parse_force_entities(force_entities: list[str] | None) -> list[tuple[str, str, float, str]]:
    if not force_entities:
        return []

    parsed: list[tuple[str, str, float, str]] = []
    for raw in force_entities:
        if not raw:
            continue
        if ":" not in raw:
            raise ValueError(f"Forced entity must be TYPE:Name, got: {raw}")
        entity_type, name = raw.split(":", 1)
        entity_type = entity_type.strip().upper()
        name = name.strip()
        if not entity_type or not name:
            raise ValueError(f"Forced entity must be TYPE:Name, got: {raw}")
        parsed.append((name, entity_type, 1.0, "forced_ingest"))

    seen: set[tuple[str, str]] = set()
    unique: list[tuple[str, str, float, str]] = []
    for name, entity_type, confidence, evidence in parsed:
        key = (name.lower(), entity_type)
        if key in seen:
            continue
        seen.add(key)
        unique.append((name, entity_type, confidence, evidence))

    return unique


def chunk_text_by_chars(text: str, chunk_size: int = 20000, overlap: int = 500) -> list[str]:
    """
    Chunk text by character count with overlap.

    Simple chunking strategy that tries to break at paragraph boundaries.
    Uses character count as proxy for tokens (roughly 1 token = 4 chars).

    Args:
        text: Text to chunk
        chunk_size: Max characters per chunk (default 20k chars ≈ 5k tokens)
        overlap: Character overlap between chunks (for context continuity)

    Returns:
        List of text chunks
    """
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        # Find end of chunk
        end = start + chunk_size

        if end >= len(text):
            # Last chunk
            chunks.append(text[start:])
            break

        # Try to break at paragraph boundary (double newline)
        break_point = text.rfind("\n\n", start, end)
        if break_point == -1:
            # Try single newline
            break_point = text.rfind("\n", start, end)
        if break_point == -1:
            # Try space
            break_point = text.rfind(" ", start, end)
        if break_point == -1:
            # Hard break
            break_point = end

        chunks.append(text[start:break_point])

        # Next chunk starts with overlap
        start = max(start + 1, break_point - overlap)

    return chunks


def extract_memories_from_chunk(
    chunk: str,
    model: str,
    min_confidence: float = 0.6,
    temporal_hints: TemporalHints | None = None,
) -> list[ExtractedMemory]:
    """
    Extract memories from a text chunk using LLM.

    Args:
        chunk: Text chunk to extract from
        model: LLM model to use
        min_confidence: Minimum confidence threshold
        temporal_hints: Optional temporal context for this chunk

    Returns:
        List of ExtractedMemory objects with temporal hints

    Raises:
        ValueError: If LLM returns invalid JSON
    """
    # Load and substitute prompt
    prompts = load_prompts()
    template = prompts.get("extract_memories_from_session")

    if not template:
        raise ValueError("'extract_memories_from_session' prompt not found in prompts.yaml")

    prompt = substitute_tokens(template, content=chunk)

    # Call LLM with schema for structured output
    try:
        result = call_llm(prompt, model=model, schema=MemoryExtractionResult)
    except Exception as e:
        raise ValueError(f"LLM call failed: {e}")

    # Convert schema response to ExtractedMemory objects
    memories = []
    for memory_schema in result.memories:
        content = memory_schema.content.strip()
        confidence = memory_schema.confidence
        rationale = memory_schema.rationale.strip()

        # Apply confidence threshold
        if confidence < min_confidence:
            continue

        # Skip empty or too-short content
        if not content or len(content) < 10:
            continue

        # Extract entities from schema
        entities = [
            (
                entity.name.strip(),
                entity.type.strip().upper(),
                entity.confidence,
                entity.evidence.strip(),
            )
            for entity in memory_schema.entities
        ]

        # Attach temporal hints to each extracted memory
        t_valid_hint = None
        temporal_stability_hint = "unknown"
        temporal_evidence = None

        if temporal_hints:
            t_valid_hint = temporal_hints.t_valid
            # Use LLM-classified stability, fallback to parser hints
            temporal_stability_hint = memory_schema.temporal_stability
            if not temporal_stability_hint or temporal_stability_hint == "unknown":
                temporal_stability_hint = temporal_hints.stability
            temporal_evidence = temporal_hints.evidence

        # If no temporal hints from parser, use LLM classification only
        if not temporal_hints and memory_schema.temporal_stability:
            temporal_stability_hint = memory_schema.temporal_stability

        memories.append(
            ExtractedMemory(
                content=content,
                confidence=confidence,
                rationale=rationale,
                entities=entities,
                t_valid_hint=t_valid_hint,
                temporal_stability_hint=temporal_stability_hint,
                temporal_evidence=temporal_evidence,
            )
        )

    return memories


def generate_summary(
    memories: list[MemoryNode],
    model: str,
    source_label: str,
    ingest_run_id: str,
    prompt_name: str = "summary_v1",
) -> SummaryResult:
    """
    Generate summary from extracted memories using LLM (M4).

    Args:
        memories: List of MemoryNode objects to summarize
        model: LLM model to use
        source_label: Human-readable source label (filename/session name)
        ingest_run_id: Ingest run identifier (artifact_ref)
        prompt_name: Name of prompt template in prompts.yaml (default: summary_v1)

    Returns:
        SummaryResult with structured summary data

    Raises:
        ValueError: If LLM returns invalid response or prompts not found
    """
    from vestig.core.entity_extraction import call_llm, load_prompts, substitute_tokens

    # Load and get prompt
    prompts = load_prompts()
    summary_prompt = prompts.get(prompt_name)

    if not summary_prompt:
        raise ValueError(f"'{prompt_name}' prompt not found in prompts.yaml")

    # Format memories list for prompt
    memory_items_list = []
    for mem in memories:
        # Truncate long memories for prompt efficiency (keep under 500 chars)
        content = mem.content[:500] + "..." if len(mem.content) > 500 else mem.content
        memory_items_list.append(f"[{mem.id}] {content}")

    memory_items_text = "\n\n".join(memory_items_list)

    # Substitute tokens in prompt
    prompt = substitute_tokens(
        summary_prompt,
        source_label=source_label,
        ingest_run_id=ingest_run_id,
        memory_items=memory_items_text,
    )

    # Call LLM with schema
    try:
        result = call_llm(prompt, model=model, schema=SummaryResult)
        return result
    except Exception as e:
        raise ValueError(f"Summary generation failed: {e}")


def commit_summary(
    summary_result: SummaryResult,
    memory_ids: list[str],
    artifact_ref: str,
    source_label: str,
    storage: MemoryStorage,
    embedding_engine: EmbeddingEngine,
    event_storage: MemoryEventStorage | None = None,
) -> str | None:
    """
    Commit summary node and create SUMMARIZES edges (M4).

    Idempotency: Uses deterministic lookup by (artifact_ref in metadata)
    to prevent duplicate summaries on re-runs.

    Args:
        summary_result: Generated summary from LLM
        memory_ids: List of memory IDs being summarized
        artifact_ref: Source artifact reference (filename/session ID)
        source_label: Human-readable source label
        storage: Storage instance
        embedding_engine: Embedding engine
        event_storage: Optional event storage

    Returns:
        Summary memory ID if created/found, None if skipped
    """
    from datetime import datetime, timezone
    import hashlib
    import uuid
    from vestig.core.models import MemoryNode, EdgeNode, EventNode

    # M4: Check if summary already exists for this artifact (idempotency)
    existing_summary = storage.get_summary_for_artifact(artifact_ref)
    if existing_summary:
        print(f"  Summary already exists: {existing_summary.id} (idempotent)")
        return existing_summary.id

    # Format summary content from bullets
    summary_data = summary_result.summary
    content_lines = [
        f"# {summary_data.title}",
        "",
        f"**Overview:** {summary_data.overview}",
        "",
        "**Key Insights:**",
    ]

    for bullet in summary_data.bullets:
        # Citations are stored as SUMMARIZES edges, not in content
        content_lines.append(f"- {bullet.text}")

    if summary_data.themes:
        content_lines.append("")
        content_lines.append(f"**Themes:** {', '.join(summary_data.themes)}")

    if summary_data.open_questions:
        content_lines.append("")
        content_lines.append("**Open Questions:**")
        for q in summary_data.open_questions:
            content_lines.append(f"- {q}")

    summary_content = "\n".join(content_lines)

    # Create summary memory node
    summary_id = f"mem_{uuid.uuid4()}"

    # Generate embedding for summary
    embedding = embedding_engine.embed_text(summary_content)

    # Compute content hash
    content_hash = hashlib.sha256(summary_content.encode("utf-8")).hexdigest()

    # Build metadata
    metadata = {
        "source": "summary_generation",
        "artifact_ref": artifact_ref,
        "source_label": source_label,
        "scope": summary_data.scope,
        "title": summary_data.title,
        "themes": summary_data.themes,
        "memory_count": len(memory_ids),
        "summarized_ids": memory_ids,
    }

    now = datetime.now(timezone.utc).isoformat()

    # Create MemoryNode with kind=SUMMARY
    summary_node = MemoryNode(
        id=summary_id,
        content=summary_content,
        content_embedding=embedding,
        content_hash=content_hash,
        created_at=now,
        metadata=metadata,
        t_valid=now,
        t_invalid=None,
        t_created=now,
        t_expired=None,
        temporal_stability="static",  # Summaries are snapshots
        last_seen_at=None,
        reinforce_count=0,
    )

    # Atomic transaction for summary + edges + event
    with storage.conn:
        # Store summary with kind=SUMMARY
        storage.store_memory(summary_node, kind="SUMMARY")

        # Create SUMMARIZES edges (Summary → Memory)
        for memory_id in memory_ids:
            edge = EdgeNode.create(
                from_node=summary_id,
                to_node=memory_id,
                edge_type="SUMMARIZES",
                weight=1.0,
                confidence=None,  # Summaries don't have confidence scores
                evidence=f"summary_of_{len(memory_ids)}_memories",
            )
            storage.store_edge(edge)

        # Log SUMMARY_CREATED event
        if event_storage:
            event = EventNode.create(
                memory_id=summary_id,
                event_type="SUMMARY_CREATED",
                source="summary_generation",
                artifact_ref=artifact_ref,
                payload={
                    "memory_count": len(memory_ids),
                    "summarized_ids": memory_ids,
                    "title": summary_data.title,
                    "themes": summary_data.themes,
                },
            )
            event_storage.add_event(event)

    print(f"  Summary created: {summary_id} (summarizes {len(memory_ids)} memories)")
    return summary_id


def ingest_document(
    document_path: str,
    storage: MemoryStorage,
    embedding_engine: EmbeddingEngine,
    extraction_model: str,
    event_storage: MemoryEventStorage | None = None,
    m4_config: dict[str, Any] | None = None,
    prompts_config: dict[str, Any] | None = None,
    chunk_size: int = 20000,
    chunk_overlap: int = 500,
    min_confidence: float = 0.6,
    source: str = "document_ingest",
    source_format: str = "auto",
    format_config: dict[str, Any] | None = None,
    force_entities: list[str] | None = None,
    verbose: bool = False,
) -> IngestionResult:
    """
    Ingest document by extracting memories with LLM and committing them.

    Args:
        document_path: Path to document file
        storage: Storage instance
        embedding_engine: Embedding engine
        event_storage: Optional event storage
        m4_config: Optional M4 config (for entity extraction)
        prompts_config: Optional prompts config (for selecting prompt versions)
        chunk_size: Characters per chunk (default 20k ≈ 5k tokens)
        chunk_overlap: Character overlap between chunks
        extraction_model: LLM model for memory extraction
        min_confidence: Minimum confidence for extracted memories
        source: Source tag for committed memories
        source_format: Input format (auto|plain|claude-session)
        format_config: Format-specific configuration
        force_entities: Entity list to attach to every memory
        verbose: Print detailed extraction output

    Returns:
        IngestionResult with statistics

    Raises:
        FileNotFoundError: If document doesn't exist
        ValueError: If extraction fails
    """
    # Read document
    path = Path(document_path)
    if not path.exists():
        raise FileNotFoundError(f"Document not found: {document_path}")

    raw_text = path.read_text(encoding="utf-8")

    text, resolved_format, document_temporal_hints = normalize_document_text(
        raw_text,
        source_format=source_format,
        format_config=format_config,
        path=path,
    )
    forced_entities = parse_force_entities(force_entities)
    if not text.strip():
        raise ValueError(f"No ingestible content found in {document_path}")
    if verbose:
        print(f"Normalized input using format: {resolved_format}")
        print(f"Temporal extraction: {document_temporal_hints.extraction_method}")
        if document_temporal_hints.t_valid:
            print(f"  t_valid: {document_temporal_hints.t_valid}")
        if document_temporal_hints.evidence:
            print(f"  Evidence: {document_temporal_hints.evidence}")

    # Chunk text
    if resolved_format == "claude-session":
        chunks_with_hints = extract_claude_session_chunks(
            raw_text,
            format_config or {},
            chunk_size,
            chunk_overlap,
        )
        chunks = [chunk for chunk, _ in chunks_with_hints]
    else:
        chunks = chunk_text_by_chars(text, chunk_size=chunk_size, overlap=chunk_overlap)
        chunks_with_hints = [(chunk, document_temporal_hints) for chunk in chunks]

    print(f"Processing {path.name}: {len(text):,} chars → {len(chunks)} chunks")

    # Track results
    memories_extracted = 0
    memories_committed = 0
    memories_deduplicated = 0
    errors = []

    # Track entities before and after
    entities_before = len(storage.get_all_entities())

    # Process each chunk
    for i, (chunk, chunk_hints) in enumerate(chunks_with_hints, 1):
        print(f"  Chunk {i}/{len(chunks)}: ", end="", flush=True)

        try:
            # Extract memories from chunk
            extracted = extract_memories_from_chunk(
                chunk,
                model=extraction_model,
                min_confidence=min_confidence,
                temporal_hints=chunk_hints,
            )

            print(f"{len(extracted)} memories extracted", flush=True)
            memories_extracted += len(extracted)

            # Show extracted memories in verbose mode
            if verbose and extracted:
                for idx, memory in enumerate(extracted, 1):
                    print(f"    Memory {idx}:")
                    print(
                        f"      Content: {memory.content[:100]}..."
                        if len(memory.content) > 100
                        else f"      Content: {memory.content}"
                    )
                    print(f"      Confidence: {memory.confidence:.2f}")
                    print(f"      Rationale: {memory.rationale}")
                    if memory.entities:
                        print(f"      Entities ({len(memory.entities)}):")
                        for name, entity_type, conf, evidence in memory.entities:
                            evid_str = (
                                f', evidence="{evidence[:50]}..."'
                                if len(evidence) > 50
                                else f', evidence="{evidence}"'
                            )
                            print(
                                f"        - {name} ({entity_type}, confidence={conf:.2f}{evid_str})"
                            )

            # Commit each memory
            for idx, memory in enumerate(extracted, 1):
                try:
                    combined_entities = []
                    if memory.entities:
                        combined_entities.extend(memory.entities)
                    if forced_entities:
                        combined_entities.extend(forced_entities)
                    if combined_entities:
                        seen_entities: set[tuple[str, str]] = set()
                        deduped_entities = []
                        for name, entity_type, confidence, evidence in combined_entities:
                            key = (name.lower(), entity_type)
                            if key in seen_entities:
                                continue
                            seen_entities.add(key)
                            deduped_entities.append((name, entity_type, confidence, evidence))
                        combined_entities = deduped_entities

                    outcome = commit_memory(
                        content=memory.content,
                        storage=storage,
                        embedding_engine=embedding_engine,
                        source=source,
                        event_storage=event_storage,
                        m4_config=m4_config,
                        artifact_ref=path.name,
                        pre_extracted_entities=combined_entities or None,
                        temporal_hints=memory,  # Pass ExtractedMemory with temporal fields
                    )

                    if outcome.outcome == "INSERTED_NEW":
                        memories_committed += 1

                        # Show entities in verbose mode
                        if verbose:
                            # Get MENTIONS edges for this memory
                            edges = storage.get_edges_from_memory(
                                outcome.memory_id, edge_type="MENTIONS", include_expired=False
                            )
                            if edges:
                                print(f"    Memory {idx} - Entities committed ({len(edges)}):")
                                for edge in edges:
                                    entity = storage.get_entity(edge.to_node)
                                    if entity:
                                        conf_str = (
                                            f", confidence={edge.confidence:.2f}"
                                            if edge.confidence
                                            else ""
                                        )
                                        evid_str = (
                                            f', evidence="{edge.evidence[:50]}..."'
                                            if edge.evidence and len(edge.evidence) > 50
                                            else f', evidence="{edge.evidence}"'
                                            if edge.evidence
                                            else ""
                                        )
                                        print(
                                            f"      - {entity.canonical_name} ({entity.entity_type}{conf_str}{evid_str})"
                                        )
                    else:
                        memories_deduplicated += 1

                except Exception as e:
                    error_msg = f"Failed to commit memory: {e}"
                    errors.append(error_msg)
                    print(f"    Error: {error_msg}")

        except Exception as e:
            error_msg = f"Chunk {i} extraction failed: {e}"
            errors.append(error_msg)
            print(f"Error: {error_msg}")

    # Track entities created
    entities_after = len(storage.get_all_entities())
    entities_created = entities_after - entities_before

    # M4: Generate summary if ≥5 memories committed
    if memories_committed >= 5 and extraction_model:
        print(f"\nGenerating summary ({memories_committed} memories extracted)...")

        try:
            # Get all memories committed during this ingest (by artifact_ref)
            # Query events to find memories from this ingest run
            cursor = storage.conn.execute(
                """
                SELECT DISTINCT m.id, m.content, m.content_embedding, m.content_hash,
                       m.created_at, m.metadata, m.t_valid, m.t_invalid,
                       m.t_created, m.t_expired, m.temporal_stability,
                       m.last_seen_at, m.reinforce_count
                FROM memories m
                JOIN memory_events e ON e.memory_id = m.id
                WHERE e.artifact_ref = ?
                  AND e.event_type = 'ADD'
                  AND m.kind = 'MEMORY'
                ORDER BY m.created_at ASC
                """,
                (path.name,),
            )

            import json

            committed_memories = []
            for row in cursor.fetchall():
                committed_memories.append(
                    MemoryNode(
                        id=row[0],
                        content=row[1],
                        content_embedding=json.loads(row[2]),
                        content_hash=row[3],
                        created_at=row[4],
                        metadata=json.loads(row[5]),
                        t_valid=row[6],
                        t_invalid=row[7],
                        t_created=row[8],
                        t_expired=row[9],
                        temporal_stability=row[10] or "unknown",
                        last_seen_at=row[11],
                        reinforce_count=row[12] or 0,
                    )
                )

            if len(committed_memories) >= 5:
                # Get summary prompt name from config (default: summary_v1)
                prompt_name = "summary_v1"
                if prompts_config:
                    prompt_name = prompts_config.get("summary", "summary_v1")

                # Generate summary
                summary_result = generate_summary(
                    memories=committed_memories,
                    model=extraction_model,
                    source_label=path.name,
                    ingest_run_id=path.name,
                    prompt_name=prompt_name,
                )

                # Commit summary with edges
                summary_id = commit_summary(
                    summary_result=summary_result,
                    memory_ids=[m.id for m in committed_memories],
                    artifact_ref=path.name,
                    source_label=path.name,
                    storage=storage,
                    embedding_engine=embedding_engine,
                    event_storage=event_storage,
                )

                if verbose and summary_id:
                    print(f"\n  Summary title: {summary_result.summary.title}")
                    print(f"  Themes: {', '.join(summary_result.summary.themes)}")

        except Exception as e:
            error_msg = f"Summary generation failed: {e}"
            errors.append(error_msg)
            print(f"  Warning: {error_msg}")

    return IngestionResult(
        document_path=document_path,
        chunks_processed=len(chunks),
        memories_extracted=memories_extracted,
        memories_committed=memories_committed,
        memories_deduplicated=memories_deduplicated,
        entities_created=entities_created,
        errors=errors,
    )


---
src/vestig/core/models.py
---
"""Data models for Vestig"""

import hashlib
import string
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any


@dataclass
class MemoryNode:
    """Memory node with M2 dedupe support and M3 temporal fields"""

    id: str
    content: str
    content_embedding: list[float]
    content_hash: str  # SHA256 of normalized content (M2: dedupe)
    created_at: str  # ISO 8601 timestamp
    metadata: dict[str, Any] = field(default_factory=dict)

    # M3: Bi-temporal fields
    t_valid: str | None = None  # When fact became true (event time)
    t_invalid: str | None = None  # When fact stopped being true (event time)
    t_created: str | None = None  # When we learned it (transaction time)
    t_expired: str | None = None  # When deprecated/superseded
    temporal_stability: str = "unknown"  # "static" | "dynamic" | "ephemeral" | "unknown"

    # M3: Reinforcement tracking (cached from events)
    last_seen_at: str | None = None  # Most recent reinforcement
    reinforce_count: int = 0  # Total reinforcement events

    @classmethod
    def create(
        cls,
        memory_id: str,
        content: str,
        embedding: list[float],
        source: str = "manual",
        tags: list[str] = None,
        content_hash: str | None = None,  # M3 FIX: Allow passing pre-computed hash
        t_valid_hint: str | None = None,  # Temporal hint: when fact became true
        temporal_stability_hint: str | None = None,  # Temporal hint: static/dynamic/ephemeral/unknown
    ) -> "MemoryNode":
        """
        Create a new memory node with M3 temporal initialization.

        Args:
            memory_id: Unique identifier (e.g., mem_uuid)
            content: Memory content text (normalized)
            embedding: Content embedding vector
            source: Source of the memory (manual, hook, batch)
            tags: Optional tags for filtering
            content_hash: Pre-computed content hash (optional, computed if not provided)
            t_valid_hint: Optional temporal hint for when fact became true
            temporal_stability_hint: Optional stability classification (static/dynamic/ephemeral/unknown)

        Returns:
            MemoryNode instance
        """
        # Compute content hash if not provided (backward compatibility)
        if content_hash is None:
            content_hash = hashlib.sha256(content.encode("utf-8")).hexdigest()

        # Build metadata
        metadata = {"source": source}
        if tags:
            metadata["tags"] = tags

        # M3: Initialize temporal fields
        now = datetime.now(timezone.utc).isoformat()

        # Use temporal hints if provided, otherwise default to now
        t_valid = t_valid_hint if t_valid_hint else now
        temporal_stability = temporal_stability_hint if temporal_stability_hint else "unknown"

        return cls(
            id=memory_id,
            content=content,
            content_embedding=embedding,
            content_hash=content_hash,
            created_at=now,
            metadata=metadata,
            # M3: Bi-temporal initialization with hints
            t_valid=t_valid,  # Use hint or fallback to now
            t_invalid=None,  # Not yet invalidated
            t_created=now,  # Transaction time is always now (when we learned it)
            t_expired=None,  # Not deprecated
            temporal_stability=temporal_stability,  # Use hint or fallback to "unknown"
            last_seen_at=None,  # No reinforcement yet
            reinforce_count=0,  # No reinforcement yet
        )


@dataclass
class EntityNode:
    """Entity node (M4: Graph Layer)

    Represents a canonical entity extracted from memories.
    Entities are deduplicated via norm_key.
    """

    id: str  # ent_<uuid>
    entity_type: str  # PERSON | ORG | SYSTEM | PROJECT | PLACE (from config)
    canonical_name: str  # Canonical form of entity name
    norm_key: str  # Normalization key for deduplication (type:normalized_name)
    created_at: str  # ISO 8601 timestamp
    expired_at: str | None = None  # When entity was merged/deprecated
    merged_into: str | None = None  # ID of entity this was merged into

    @classmethod
    def create(
        cls,
        entity_type: str,
        canonical_name: str,
        entity_id: str | None = None,
    ) -> "EntityNode":
        """
        Create a new entity node with computed norm_key.

        Args:
            entity_type: Type of entity (must be in config.allowed_types)
            canonical_name: Canonical form of entity name
            entity_id: Optional entity ID (generated if not provided)

        Returns:
            EntityNode instance
        """
        if entity_id is None:
            entity_id = f"ent_{uuid.uuid4()}"

        # Compute norm_key deterministically
        norm_key = compute_norm_key(canonical_name, entity_type)

        now = datetime.now(timezone.utc).isoformat()

        return cls(
            id=entity_id,
            entity_type=entity_type,
            canonical_name=canonical_name,
            norm_key=norm_key,
            created_at=now,
            expired_at=None,
            merged_into=None,
        )


@dataclass
class EdgeNode:
    """Edge node (M4: Graph Layer)

    Represents a relationship between nodes with bi-temporal tracking.
    Edge types: MENTIONS (Memory→Entity), RELATED (Memory→Memory)
    """

    edge_id: str  # edge_<uuid>
    from_node: str  # Source node ID (mem_* or ent_*)
    to_node: str  # Target node ID (mem_* or ent_*)
    edge_type: str  # MENTIONS | RELATED (enforced)
    weight: float  # Edge weight (1.0 default, or similarity score)

    # M4: LLM extraction metadata
    confidence: float | None = None  # Extraction confidence (0.0-1.0)
    evidence: str | None = None  # Short explanation (max 200 chars)

    # M4: Bi-temporal fields (same as entities)
    t_valid: str | None = None  # When relationship became true
    t_invalid: str | None = None  # When relationship stopped being true
    t_created: str | None = None  # When we learned about this relationship
    t_expired: str | None = None  # When edge was invalidated

    @classmethod
    def create(
        cls,
        from_node: str,
        to_node: str,
        edge_type: str,
        weight: float = 1.0,
        confidence: float | None = None,
        evidence: str | None = None,
        edge_id: str | None = None,
    ) -> "EdgeNode":
        """
        Create a new edge node with bi-temporal initialization.

        Args:
            from_node: Source node ID
            to_node: Target node ID
            edge_type: MENTIONS or RELATED (validated)
            weight: Edge weight (default 1.0)
            confidence: Optional extraction confidence (0.0-1.0)
            evidence: Optional short explanation
            edge_id: Optional edge ID (generated if not provided)

        Returns:
            EdgeNode instance

        Raises:
            ValueError: If edge_type is invalid
        """
        # Enforce edge type constraints
        allowed_edge_types = {"MENTIONS", "RELATED", "SUMMARIZES"}
        if edge_type not in allowed_edge_types:
            raise ValueError(f"Invalid edge_type: {edge_type}. Allowed: {allowed_edge_types}")

        if edge_id is None:
            edge_id = f"edge_{uuid.uuid4()}"

        # Truncate evidence if too long
        if evidence and len(evidence) > 200:
            evidence = evidence[:197] + "..."

        now = datetime.now(timezone.utc).isoformat()

        return cls(
            edge_id=edge_id,
            from_node=from_node,
            to_node=to_node,
            edge_type=edge_type,
            weight=weight,
            confidence=confidence,
            evidence=evidence,
            # Bi-temporal initialization
            t_valid=now,
            t_invalid=None,
            t_created=now,
            t_expired=None,
        )


@dataclass
class EventNode:
    """Memory lifecycle event (M3)"""

    event_id: str  # evt_<uuid>
    memory_id: str  # FK to memories table
    event_type: (
        str  # ADD | REINFORCE_EXACT | REINFORCE_NEAR | DEPRECATE | SUPERSEDE | ENTITY_EXTRACTED | SUMMARY_CREATED
    )
    occurred_at: str  # UTC timestamp (ISO 8601)
    source: str  # manual | hook | import | batch | llm | summary_generation
    actor: str | None = None  # User/agent identifier
    artifact_ref: str | None = None  # Session ID, filename, etc.
    payload: dict[str, Any] = field(default_factory=dict)  # Event details

    @classmethod
    def create(
        cls,
        memory_id: str,
        event_type: str,
        source: str = "manual",
        actor: str | None = None,
        artifact_ref: str | None = None,
        payload: dict[str, Any] | None = None,
    ) -> "EventNode":
        """Create new event with generated ID and timestamp"""
        return cls(
            event_id=f"evt_{uuid.uuid4()}",
            memory_id=memory_id,
            event_type=event_type,
            occurred_at=datetime.now(timezone.utc).isoformat(),
            source=source,
            actor=actor,
            artifact_ref=artifact_ref,
            payload=payload or {},
        )


# M4: Utility Functions


def compute_norm_key(text: str, entity_type: str) -> str:
    """
    Compute normalization key for entity deduplication.

    Deterministic canonicalization:
    - Lowercase
    - Collapse whitespace
    - Strip leading/trailing punctuation
    - Prefix with entity_type for scoped deduplication

    Args:
        text: Entity name/text to normalize
        entity_type: Entity type (for scoping)

    Returns:
        Normalized key in format "TYPE:normalized_text"

    Examples:
        >>> compute_norm_key("Alice Smith", "PERSON")
        'PERSON:alice smith'
        >>> compute_norm_key("PostgreSQL", "SYSTEM")
        'SYSTEM:postgresql'
        >>> compute_norm_key("  Dr. Alice  ", "PERSON")
        'PERSON:dr alice'
    """
    # Lowercase and collapse whitespace
    normalized = " ".join(text.lower().strip().split())

    # Strip leading/trailing punctuation
    normalized = normalized.strip(string.punctuation)

    # Prefix with entity type for scoped deduplication
    return f"{entity_type}:{normalized}"


---
src/vestig/core/prompts.yaml
---
# Vestig LLM Prompts
# Use {{token}} for variable substitution
# Supports both string format (legacy) and dict format with system/user keys (M4+)

# Session Ingestion: Extract memories from conversation transcripts
extract_memories_from_session:
  description: >
    Extract discrete, factual memories from conversation transcripts with entity extraction
    and temporal stability classification. Memories should be self-contained and substantive.
  system: |
    You are a memory extraction engine for an agent memory system.
    Your task is to extract discrete, factual memories from conversation transcripts.
    Each memory must be self-contained and include entity extraction and temporal classification.
    Output MUST be valid JSON matching the schema exactly.
    Do NOT extract greetings, small talk, or procedural exchanges - only substantive content.
  user: |
    Extract memories from this conversation chunk.

    GUIDELINES FOR MEMORIES:
    - One clear fact, decision, insight, or knowledge point per memory
    - Self-contained with sufficient context for standalone understanding
    - Substantive content only: technical details, decisions, learnings
    - Skip greetings, small talk, procedural exchanges
    - Preserve key context (who, what, when relevant)
    - Confidence reflects clarity and factuality

    GUIDELINES FOR ENTITIES:
    - Extract entities mentioned in each memory
    - Entity types: PERSON, ORG, SYSTEM, PROJECT, PLACE, SKILL, TOOL, FILE, CONCEPT
    - Only extract entities that are clearly mentioned in the memory
    - Provide text evidence supporting each entity
    - Confidence reflects certainty this is the correct entity type

    GUIDELINES FOR TEMPORAL STABILITY:
    Classify each memory's temporal stability (how it changes over time):
    - "static": Permanent facts that never change
      Examples: dates of birth, historical events, past actions/decisions, completed migrations
      Indicators: "was born", "happened on", "decided to", "renamed", "created in"
    - "dynamic": Current state that may change in the future
      Examples: job titles, system configurations, project status, current locations, version numbers
      Indicators: "is currently", "status:", "works as", "configured to", "running on"
    - "ephemeral": Expected to change soon (hours/days), only when explicitly indicated
      Examples: incident ongoing, temporary workaround, on-call this week, today's deployment status
      Indicators: "today", "this week", "temporary", "incident ongoing", "until Friday"
    - "unknown": Mixed content or uncertain classification
      Examples: memories with both static and dynamic elements, or unclear permanence

    CONVERSATION CHUNK:
    {{content}}

    OUTPUT JSON SCHEMA (must match exactly):
    {
      "memories": [
        {
          "content": "self-contained memory with context",
          "confidence": 0.0-1.0,
          "rationale": "why this matters",
          "temporal_stability": "static|dynamic|ephemeral|unknown",
          "entities": [
            {
              "name": "entity name",
              "type": "PERSON|ORG|SYSTEM|PROJECT|PLACE|SKILL|TOOL|FILE|CONCEPT",
              "confidence": 0.0-1.0,
              "evidence": "text snippet supporting this entity"
            }
          ]
        }
      ]
    }

    Return {"memories": []} if none found.

# M4: Document/Session Summary Generation
# Create grounded summaries from extracted MEMORY nodes
summary_v1:
  description: >
    Create a grounded document/session summary from extracted MEMORY nodes.
    The summary MUST ONLY use provided memory items. 
  system: |
    You are a careful summarization engine for a memory graph.
    You must ONLY use the provided MEMORY items as evidence.
    Do NOT introduce new facts, names, numbers, or claims.
    If information is missing, say so in "open_questions" rather than guessing.
    Output MUST be valid JSON matching the schema.
  user: |
    Create a SUMMARY node for this ingest run.

    Source label: {{source_label}}
    Ingest run id: {{ingest_run_id}}

    MEMORY items (each is atomic; treat them as ground truth):
    {{memory_items}}

    OUTPUT JSON SCHEMA (must match exactly):
    {
      "summary": {
        "title": "string - short descriptive title for this document/session",
        "scope": "INGEST_RUN",
        "overview": "string - 2-4 sentences in plain language",
        "bullets": [
          {
            "text": "string - one key insight or fact",
            "memory_ids": ["mem_xxx", "mem_yyy"]
          }
        ],
        "themes": ["string - short tags, single words or short phrases"],
        "open_questions": ["string - genuine gaps or ambiguities if any"]
      }
    }

    CONSTRAINTS:
    - overview: 2–4 sentences, plain language.
    - bullets: 6–12 bullets. Each bullet must cite 1–4 memory_ids that support it.
    - themes: 3–8 short tags (single words or short phrases).
    - open_questions: empty list unless there are genuine gaps/ambiguities.
    - Do not quote long passages. Do not add commentary about your process.
# Create embedding-optimized summaries from MEMORY nodes
summary_v2:
  description: >
    Generate dense, searchable summaries from MEMORY nodes for embedding retrieval.
    Output must be grounded in provided memories only.
  system: |
    Generate embedding-optimized summaries from provided MEMORY items.
    Rules: Use only provided memories. No invented facts. Prioritize searchable keywords and phrases.
    Output valid JSON matching schema.
  user: |
    Summarize for embedding search.

    Source: {{source_label}}
    Run: {{ingest_run_id}}

    MEMORIES:
    {{memory_items}}

    SCHEMA:
    {
      "summary": {
        "title": "string - descriptive, keyword-rich (8-12 words)",
        "overview": "string - dense 2-3 sentences with key entities, actions, outcomes",
        "bullets": [
          {
            "text": "string - specific insight with entities/numbers",
            "memory_ids": ["mem_xxx"]
          }
        ],
        "themes": ["string - searchable terms"],
        "gaps": ["string - missing info"] 
      }
    }

    OPTIMIZE FOR SEARCH:
    - title: Include main entities, topics, or outcomes
    - overview: Front-load key terms, entities, dates, locations
    - bullets: 6-10 items. Cite 1-3 memory_ids each. Use specific nouns/verbs.
    - themes: 4-8 terms optimized for semantic search
    - gaps: Empty unless critical information missing

    Output JSON only.

---
src/vestig/core/retrieval.py
---
"""Retrieval logic for M1 (brute-force cosine similarity) with M3 TraceRank"""

import time
from datetime import datetime, timezone
from typing import TYPE_CHECKING

import numpy as np

from vestig.core.embeddings import EmbeddingEngine
from vestig.core.models import MemoryNode
from vestig.core.storage import MemoryStorage

if TYPE_CHECKING:
    from vestig.core.event_storage import MemoryEventStorage
    from vestig.core.tracerank import TraceRankConfig


def cosine_similarity(a: list[float], b: list[float]) -> float:
    """
    Compute cosine similarity between two vectors.

    Args:
        a: First vector
        b: Second vector

    Returns:
        Cosine similarity score (0-1, higher is more similar)
    """
    a_arr = np.array(a)
    b_arr = np.array(b)

    norm_a = np.linalg.norm(a_arr)
    norm_b = np.linalg.norm(b_arr)

    # Guard against zero vectors (avoid NaN)
    if norm_a == 0 or norm_b == 0:
        return 0.0

    return float(np.dot(a_arr, b_arr) / (norm_a * norm_b))


def search_memories(
    query: str,
    storage: MemoryStorage,
    embedding_engine: EmbeddingEngine,
    limit: int = 5,
    event_storage: MemoryEventStorage | None = None,  # M3
    tracerank_config: TraceRankConfig | None = None,  # M3
    include_expired: bool = False,  # M3
    show_timing: bool = False,  # Performance instrumentation
) -> list[tuple[MemoryNode, float]]:
    """
    Search memories by semantic similarity (brute-force) with M3 TraceRank.

    Args:
        query: Search query text
        storage: Storage instance
        embedding_engine: Embedding engine instance
        limit: Number of top results to return
        event_storage: Optional event storage for TraceRank (M3)
        tracerank_config: Optional TraceRank configuration (M3)
        include_expired: Include deprecated/expired memories (M3)
        show_timing: Display performance timing breakdown

    Returns:
        List of (MemoryNode, final_score) tuples, sorted by score descending
        final_score = semantic_score * tracerank_multiplier
    """
    t_start = time.perf_counter()
    timings = {}

    # Generate query embedding
    t0 = time.perf_counter()
    query_embedding = embedding_engine.embed_text(query)
    timings['1_embedding_generation'] = time.perf_counter() - t0

    # Load memories (active only or all) - M3
    t0 = time.perf_counter()
    if include_expired or event_storage is None:
        all_memories = storage.get_all_memories()
    else:
        all_memories = storage.get_active_memories()
    timings['2_load_memories'] = time.perf_counter() - t0

    if not all_memories:
        if show_timing:
            print(f"\n[TIMING] Total: {(time.perf_counter() - t_start)*1000:.0f}ms (no memories)")
        return []

    # Compute semantic scores
    t0 = time.perf_counter()
    scored_memories = []
    for memory in all_memories:
        semantic_score = cosine_similarity(query_embedding, memory.content_embedding)
        scored_memories.append((memory, semantic_score))
    timings['3_semantic_scoring'] = time.perf_counter() - t0

    # M3: Apply Enhanced TraceRank if enabled
    if event_storage and tracerank_config and tracerank_config.enabled:
        from vestig.core.tracerank import compute_enhanced_multiplier

        # Compute Enhanced TraceRank for all memories
        t0 = time.perf_counter()
        tracerank_timings = {'events': 0, 'edges': 0, 'compute': 0}

        for i, (memory, semantic_score) in enumerate(scored_memories):
            # Get reinforcement events
            t1 = time.perf_counter()
            events = event_storage.get_reinforcement_events(memory.id)
            tracerank_timings['events'] += time.perf_counter() - t1

            # Get inbound edge count (graph connectivity)
            t1 = time.perf_counter()
            inbound_edges = storage.get_edges_to_memory(memory.id, include_expired=False)
            edge_count = len(inbound_edges)
            tracerank_timings['edges'] += time.perf_counter() - t1

            # Compute comprehensive multiplier
            t1 = time.perf_counter()
            multiplier = compute_enhanced_multiplier(
                memory_id=memory.id,
                temporal_stability=memory.temporal_stability,
                t_valid=memory.t_valid or memory.created_at,  # Fallback to created_at
                inbound_edge_count=edge_count,
                reinforcement_events=events,
                config=tracerank_config,
            )
            tracerank_timings['compute'] += time.perf_counter() - t1

            # Multiply semantic score by enhanced multiplier
            scored_memories[i] = (memory, semantic_score * multiplier)

        timings['4_tracerank_total'] = time.perf_counter() - t0
        timings['4a_tracerank_events'] = tracerank_timings['events']
        timings['4b_tracerank_edges'] = tracerank_timings['edges']
        timings['4c_tracerank_compute'] = tracerank_timings['compute']

    # Sort by final score descending and return top-K
    t0 = time.perf_counter()
    scored_memories.sort(key=lambda x: x[1], reverse=True)
    result = scored_memories[:limit]
    timings['5_sort_and_slice'] = time.perf_counter() - t0

    timings['TOTAL'] = time.perf_counter() - t_start

    if show_timing:
        print("\n" + "="*60)
        print("PERFORMANCE BREAKDOWN")
        print("="*60)
        for key, duration in timings.items():
            ms = duration * 1000
            pct = (duration / timings['TOTAL'] * 100) if timings['TOTAL'] > 0 else 0
            indent = "  " if key.startswith(('4a', '4b', '4c')) else ""
            print(f"{indent}{key:30s} {ms:8.0f}ms  ({pct:5.1f}%)")
        print("="*60 + "\n")

    return result


def format_search_results(results: list[tuple[MemoryNode, float]]) -> str:
    """
    Format search results for display.

    Args:
        results: List of (MemoryNode, similarity_score) tuples

    Returns:
        Formatted string for terminal output
    """
    if not results:
        return "No memories found."

    lines = []
    for memory, score in results:
        # Truncate content to 100 chars
        content_preview = memory.content[:100]
        if len(memory.content) > 100:
            content_preview += "..."

        # Parse and format timestamp
        created_dt = datetime.fromisoformat(memory.created_at.replace("Z", "+00:00"))
        created_str = created_dt.strftime("%Y-%m-%d %H:%M:%S UTC")

        lines.append(
            f"ID: {memory.id}\n"
            f"Score: {score:.4f}\n"
            f"Created: {created_str}\n"
            f"Content: {content_preview}\n"
        )

    return "\n".join(lines)


def _format_age(timestamp_str: str) -> str:
    """
    Format timestamp as human-readable age.

    Args:
        timestamp_str: ISO 8601 timestamp string

    Returns:
        Human-readable age (e.g., "3d", "2h", "45m", "just now")
    """
    try:
        timestamp = datetime.fromisoformat(timestamp_str)
        now = datetime.now(timezone.utc)

        # Ensure both are timezone-aware
        if timestamp.tzinfo is None:
            timestamp = timestamp.replace(tzinfo=timezone.utc)

        delta = now - timestamp

        # Format as compact age
        total_seconds = delta.total_seconds()

        if total_seconds < 60:
            return "just now"
        elif total_seconds < 3600:  # < 1 hour
            minutes = int(total_seconds / 60)
            return f"{minutes}m"
        elif total_seconds < 86400:  # < 1 day
            hours = int(total_seconds / 3600)
            return f"{hours}h"
        elif total_seconds < 604800:  # < 1 week
            days = int(total_seconds / 86400)
            return f"{days}d"
        elif total_seconds < 2592000:  # < 30 days
            weeks = int(total_seconds / 604800)
            return f"{weeks}w"
        else:
            months = int(total_seconds / 2592000)
            return f"{months}mo"
    except Exception:
        return "unknown"


def format_recall_results(results: list[tuple[MemoryNode, float]]) -> str:
    """
    Format recall results for agent context.

    Optimized for AI consumption with minimal metadata:
    - score (confidence)
    - age (temporal context/freshness)
    - stability (trust/reliability)

    Args:
        results: List of (MemoryNode, similarity_score) tuples

    Returns:
        Formatted string suitable for LLM context

    Format:
        (score=0.82, age=3d, stability=static)
        <content>
    """
    if not results:
        return "No memories found."

    blocks = []
    for memory, score in results:
        # Compute age from created_at
        age = _format_age(memory.created_at)

        # Get stability (default to unknown if not present)
        stability = getattr(memory, "temporal_stability", "unknown")

        # Minimal header: score, age, stability (no ID - not useful for AI)
        header = f"(score={score:.4f}, age={age}, stability={stability})"

        blocks.append(f"{header}\n{memory.content}")

    return "\n\n---\n\n".join(blocks)


def format_recall_results_with_explanation(
    results: list[tuple[MemoryNode, float]],
    event_storage: "MemoryEventStorage",
    storage: "MemoryStorage",
    tracerank_config: "TraceRankConfig",
) -> str:
    """
    Format recall results with explanations for why each memory was retrieved.

    Args:
        results: List of (MemoryNode, final_score) tuples
        event_storage: Event storage for TraceRank analysis
        storage: Memory storage for graph queries
        tracerank_config: TraceRank configuration

    Returns:
        Formatted string with explanations

    Format:
        [META] (score=0.82, age=3d, stability=static)
        Semantic match. TraceRank: 1.42x (3x reinforced, 2 conn). Static.
        [MEMORY]
        <content>
    """
    if not results:
        return "No memories found."

    blocks = []
    for memory, final_score in results:
        # Compute age from created_at
        age = _format_age(memory.created_at)

        # Get stability (default to unknown if not present)
        stability = getattr(memory, "temporal_stability", "unknown")

        # Header (same as standard format, no ID)
        header = f"(score={final_score:.4f}, age={age}, stability={stability})"

        # Generate explanation
        explanation_parts = []

        # TraceRank analysis
        try:
            from vestig.core.tracerank import compute_enhanced_multiplier

            # Get reinforcement events
            events = event_storage.get_reinforcement_events(memory.id)
            reinforcement_count = len(events)

            # Get graph connectivity (inbound edges)
            inbound_edges = storage.get_edges_to_memory(memory.id)
            edge_count = len(inbound_edges)

            # Compute TraceRank multiplier
            tracerank_mult = compute_enhanced_multiplier(
                memory_id=memory.id,
                temporal_stability=stability,
                t_valid=getattr(memory, "t_valid", None) or memory.created_at,
                inbound_edge_count=edge_count,
                reinforcement_events=events,
                config=tracerank_config,
            )

            # Build explanation (token-efficient)
            explanation_parts.append("Semantic match.")

            # Show TraceRank boost if significant
            if tracerank_mult > 1.0:
                tracerank_details = []
                if reinforcement_count > 0:
                    tracerank_details.append(f"{reinforcement_count}x reinforced")
                if edge_count > 0:
                    tracerank_details.append(f"{edge_count} conn")

                if tracerank_details:
                    details_str = ", ".join(tracerank_details)
                    explanation_parts.append(f"TraceRank: {tracerank_mult:.2f}x ({details_str}).")
                else:
                    explanation_parts.append(f"TraceRank: {tracerank_mult:.2f}x.")

            # Temporal stability note (compact)
            if stability == "dynamic":
                explanation_parts.append("Dynamic (may decay).")
            elif stability == "static":
                explanation_parts.append("Static.")

        except Exception as e:
            explanation_parts.append(f"Semantic match. (Analysis error: {e})")

        explanation = " ".join(explanation_parts)

        # Combine header, explanation, and content with clear labels
        blocks.append(f"[META] {header}\n{explanation}\n[MEMORY]\n{memory.content}")

    return "\n\n---\n\n".join(blocks)


---
src/vestig/core/schema.sql
---
-- Vestig Schema v1.0
-- Milestone: M4 (Summary Nodes)
-- Last Updated: 2025-01-XX
-- DO NOT MODIFY: This is the sovereign interface for Vestig's data model
--
-- This file defines the complete schema for fresh Vestig database creation.
-- Existing databases are migrated using additive migrations in storage.py.
--
-- Schema Evolution:
-- M1: Base memories table with content and embeddings
-- M2: Added content_hash for deduplication
-- M3: Added bi-temporal fields (t_valid, t_invalid, t_created, t_expired) and events table
-- M4: Added graph layer (entities, edges) and kind discriminator for MEMORY/SUMMARY nodes

-- =============================================================================
-- Core Tables: Memories and Events
-- =============================================================================

-- memories: Core memory nodes with bi-temporal tracking
CREATE TABLE memories (
    id TEXT PRIMARY KEY,
    content TEXT NOT NULL,
    content_embedding TEXT NOT NULL,  -- JSON-serialized embedding vector
    created_at TEXT NOT NULL,         -- ISO 8601 timestamp (when memory was created)
    metadata TEXT NOT NULL,           -- JSON-serialized metadata dict

    -- M2: Deduplication
    content_hash TEXT,                -- SHA256 of normalized content

    -- M3: Bi-temporal fields (event time)
    t_valid TEXT,                     -- When fact became true (event time)
    t_invalid TEXT,                   -- When fact stopped being true (event time)
    t_created TEXT,                   -- When we learned about it (transaction time)
    t_expired TEXT,                   -- When deprecated/superseded
    temporal_stability TEXT DEFAULT 'unknown',  -- 'static' | 'dynamic' | 'unknown'

    -- M3: Reinforcement tracking (cached from events)
    last_seen_at TEXT,                -- Most recent reinforcement timestamp
    reinforce_count INTEGER DEFAULT 0, -- Total reinforcement events

    -- M4: Node type discriminator
    kind TEXT DEFAULT 'MEMORY'        -- 'MEMORY' | 'SUMMARY'
);

-- memory_events: Event log for memory lifecycle tracking
CREATE TABLE memory_events (
    event_id TEXT PRIMARY KEY,
    memory_id TEXT NOT NULL,
    event_type TEXT NOT NULL,         -- ADD | REINFORCE_EXACT | REINFORCE_NEAR | DEPRECATE | SUPERSEDE | ENTITY_EXTRACTED | SUMMARY_CREATED
    occurred_at TEXT NOT NULL,        -- ISO 8601 timestamp (when event occurred)
    source TEXT NOT NULL,             -- manual | hook | import | batch | llm | summary_generation
    actor TEXT,                       -- User/agent identifier (optional)
    artifact_ref TEXT,                -- Session ID, filename, etc. (optional)
    payload_json TEXT NOT NULL,       -- JSON-serialized event details
    FOREIGN KEY(memory_id) REFERENCES memories(id)
);

-- =============================================================================
-- M4 Graph Layer: Entities and Edges
-- =============================================================================

-- entities: Canonical entity nodes (PERSON, ORG, SYSTEM, etc.)
CREATE TABLE entities (
    id TEXT PRIMARY KEY,
    entity_type TEXT NOT NULL,        -- PERSON | ORG | SYSTEM | PROJECT | PLACE | SKILL | TOOL | FILE | CONCEPT
    canonical_name TEXT NOT NULL,     -- Canonical form of entity name
    norm_key TEXT NOT NULL,           -- Normalization key for deduplication (type:normalized_name)
    created_at TEXT NOT NULL,         -- ISO 8601 timestamp
    expired_at TEXT,                  -- When entity was merged/deprecated
    merged_into TEXT                  -- ID of entity this was merged into
);

-- edges: Graph edges connecting memories and entities
CREATE TABLE edges (
    edge_id TEXT PRIMARY KEY,
    from_node TEXT NOT NULL,          -- Source node ID (mem_* or ent_*)
    to_node TEXT NOT NULL,            -- Target node ID (mem_* or ent_*)
    edge_type TEXT NOT NULL,          -- MENTIONS | RELATED | SUMMARIZES
    weight REAL NOT NULL,             -- Edge weight (1.0 default, or similarity score)

    -- M4: LLM extraction metadata
    confidence REAL,                  -- Extraction confidence (0.0-1.0, optional)
    evidence TEXT,                    -- Short explanation (max 200 chars, optional)

    -- M4: Bi-temporal fields (same as memories)
    t_valid TEXT,                     -- When relationship became true
    t_invalid TEXT,                   -- When relationship stopped being true
    t_created TEXT,                   -- When we learned about this relationship
    t_expired TEXT                    -- When edge was invalidated
);

-- =============================================================================
-- Indexes: Memories Table
-- =============================================================================

-- M2: Unique index on content_hash for deduplication
CREATE UNIQUE INDEX idx_content_hash
ON memories(content_hash);

-- M3: Partial index for expired memories
CREATE INDEX idx_memories_expired
ON memories(t_expired)
WHERE t_expired IS NOT NULL;

-- M4: Index for kind column (MEMORY vs SUMMARY queries)
CREATE INDEX idx_memories_kind
ON memories(kind);

-- =============================================================================
-- Indexes: Memory Events Table
-- =============================================================================

-- M3: Index for querying events by memory and time (most recent first)
CREATE INDEX idx_events_memory_time
ON memory_events(memory_id, occurred_at DESC);

-- M3: Index for filtering events by type
CREATE INDEX idx_events_type
ON memory_events(event_type);

-- =============================================================================
-- Indexes: Entities Table
-- =============================================================================

-- M4: Unique index on norm_key for entity deduplication
CREATE UNIQUE INDEX idx_entities_norm_key
ON entities(norm_key);

-- M4: Index for querying entities by type
CREATE INDEX idx_entities_type
ON entities(entity_type);

-- M4: Partial index for expired entities
CREATE INDEX idx_entities_expired
ON entities(expired_at)
WHERE expired_at IS NOT NULL;

-- =============================================================================
-- Indexes: Edges Table
-- =============================================================================

-- M4: Index for querying edges from a node (with edge type filter)
CREATE INDEX idx_edges_from_node
ON edges(from_node, edge_type);

-- M4: Index for querying edges to a node (with edge type filter)
CREATE INDEX idx_edges_to_node
ON edges(to_node, edge_type);

-- M4: Index for filtering edges by type
CREATE INDEX idx_edges_type
ON edges(edge_type);

-- M4: Partial index for high-confidence edges
CREATE INDEX idx_edges_confidence
ON edges(confidence)
WHERE confidence IS NOT NULL;

-- M4: Partial index for expired edges
CREATE INDEX idx_edges_expired
ON edges(t_expired)
WHERE t_expired IS NOT NULL;

-- M4: Unique constraint to prevent duplicate edges (belt and braces)
CREATE UNIQUE INDEX idx_edges_unique
ON edges(from_node, to_node, edge_type)
WHERE t_expired IS NULL;


---
src/vestig/core/server.py
---
"""Simple HTTP server for Vestig to avoid model reload overhead"""

import json
from http.server import BaseHTTPRequestHandler, HTTPServer
from typing import Any

from vestig.core.cli import build_runtime
from vestig.core.config import load_config
from vestig.core.retrieval import search_memories


class VestigServer:
    """Vestig server that keeps embedding model in memory"""

    def __init__(self, config_path: str, host: str = "127.0.0.1", port: int = 8765):
        """Initialize server with config.

        Args:
            config_path: Path to vestig config file
            host: Server host (default: 127.0.0.1)
            port: Server port (default: 8765)
        """
        self.config_path = config_path
        self.host = host
        self.port = port

        print(f"Loading config from {config_path}...")
        self.config = load_config(config_path)

        print("Initializing runtime (this will load the embedding model)...")
        self.storage, self.embedding_engine, self.event_storage, self.tracerank_config = (
            build_runtime(self.config)
        )
        print(f"✓ Model loaded: {self.config['embedding']['model']}")
        print(f"✓ Database: {self.config['storage']['db_path']}")

    def handle_query(self, query: str, limit: int = 5, show_timing: bool = False) -> dict[str, Any]:
        """Handle a search/recall query.

        Args:
            query: Search query text
            limit: Number of results
            show_timing: Show timing breakdown

        Returns:
            Dictionary with results and metadata
        """
        results = search_memories(
            query=query,
            storage=self.storage,
            embedding_engine=self.embedding_engine,
            limit=limit,
            event_storage=self.event_storage,
            tracerank_config=self.tracerank_config,
            show_timing=show_timing,
        )

        return {
            "query": query,
            "limit": limit,
            "count": len(results),
            "results": [
                {
                    "id": memory.id,
                    "score": float(score),
                    "content": memory.content,
                    "created_at": memory.created_at,
                    "metadata": memory.metadata,
                }
                for memory, score in results
            ],
        }

    def start(self):
        """Start the HTTP server."""

        server_instance = self

        class RequestHandler(BaseHTTPRequestHandler):
            def do_POST(self):
                if self.path == "/query":
                    # Read request body
                    content_length = int(self.headers["Content-Length"])
                    body = self.rfile.read(content_length)
                    request_data = json.loads(body.decode("utf-8"))

                    # Extract parameters
                    query = request_data.get("query")
                    limit = request_data.get("limit", 5)
                    show_timing = request_data.get("show_timing", False)

                    if not query:
                        self.send_error(400, "Missing 'query' parameter")
                        return

                    try:
                        # Process query
                        result = server_instance.handle_query(query, limit, show_timing)

                        # Send response
                        self.send_response(200)
                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        self.wfile.write(json.dumps(result).encode("utf-8"))

                    except Exception as e:
                        self.send_error(500, f"Query error: {e}")

                elif self.path == "/health":
                    # Health check endpoint
                    self.send_response(200)
                    self.send_header("Content-Type", "application/json")
                    self.end_headers()
                    response = {
                        "status": "healthy",
                        "model": server_instance.config["embedding"]["model"],
                        "db": server_instance.config["storage"]["db_path"],
                    }
                    self.wfile.write(json.dumps(response).encode("utf-8"))

                else:
                    self.send_error(404, "Endpoint not found")

            def log_message(self, format, *args):
                # Suppress default logging, we'll do our own
                pass

        server = HTTPServer((self.host, self.port), RequestHandler)
        print(f"\n{'='*60}")
        print(f"Vestig server running on http://{self.host}:{self.port}")
        print(f"{'='*60}")
        print("Endpoints:")
        print(f"  POST /query    - Search/recall memories")
        print(f"  POST /health   - Health check")
        print(f"\nPress Ctrl+C to stop")
        print(f"{'='*60}\n")

        try:
            server.serve_forever()
        except KeyboardInterrupt:
            print("\nShutting down server...")
            server.shutdown()
            self.storage.close()
            print("Server stopped.")


---
src/vestig/core/storage.py
---
"""Storage layer using SQLite with M2 dedupe support"""

import json
import sqlite3
from pathlib import Path

from vestig.core.models import EdgeNode, EntityNode, MemoryNode


class MemoryStorage:
    """SQLite storage for memory nodes with M2 dedupe"""

    def __init__(self, db_path: str):
        """
        Initialize storage.

        Args:
            db_path: Path to SQLite database file
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(self.db_path))

        # M3 FIX: Enable foreign key constraints (SQLite doesn't enforce by default)
        self.conn.execute("PRAGMA foreign_keys = ON")

        self._init_schema()
        self._validate_schema()

    def _is_fresh_database(self) -> bool:
        """Check if database is empty (no tables exist)

        Returns:
            True if this is a fresh database with no memories table
        """
        cursor = self.conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='memories'"
        )
        return cursor.fetchone() is None

    def _apply_schema_file(self) -> None:
        """Apply schema.sql for fresh database creation

        Raises:
            FileNotFoundError: If schema.sql is not found
        """
        schema_path = Path(__file__).parent / "schema.sql"
        if not schema_path.exists():
            raise FileNotFoundError(f"schema.sql not found at {schema_path}")

        schema_sql = schema_path.read_text()
        self.conn.executescript(schema_sql)
        self.conn.commit()
        # Note: Using logger here would require import at top
        # logger.info("Applied schema.sql for fresh database creation")

    def _init_schema(self):
        """Initialize or migrate database schema

        Fresh databases: Apply schema.sql as single source of truth
        Existing databases: Use migration logic for backward compatibility
        """
        if self._is_fresh_database():
            # Fresh DB: Apply schema.sql as single source of truth
            self._apply_schema_file()
        else:
            # Existing DB: Keep migration logic for backward compatibility
            self._migrate_existing_database()

    def _migrate_existing_database(self):
        """Legacy migration path for pre-M0 databases (additive migrations)"""
        # Create base table
        self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS memories (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                content_embedding TEXT NOT NULL,
                created_at TEXT NOT NULL,
                metadata TEXT NOT NULL
            )
            """
        )

        # M2: Add content_hash column if it doesn't exist (additive migration)
        cursor = self.conn.execute("PRAGMA table_info(memories)")
        columns = [row[1] for row in cursor.fetchall()]

        if "content_hash" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN content_hash TEXT")

        # M3 FIX: Always ensure unique index exists (even if column already present)
        # Create unique index on content_hash for dedupe
        self.conn.execute(
            """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_content_hash
            ON memories(content_hash)
            """
        )

        # M3: Add temporal columns (check first, then ALTER TABLE)
        cursor = self.conn.execute("PRAGMA table_info(memories)")
        columns = [row[1] for row in cursor.fetchall()]

        if "t_valid" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN t_valid TEXT")
        if "t_invalid" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN t_invalid TEXT")
        if "t_created" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN t_created TEXT")
        if "t_expired" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN t_expired TEXT")
        if "temporal_stability" not in columns:
            self.conn.execute(
                "ALTER TABLE memories ADD COLUMN temporal_stability TEXT DEFAULT 'unknown'"
            )
        if "last_seen_at" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN last_seen_at TEXT")
        if "reinforce_count" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN reinforce_count INTEGER DEFAULT 0")

        # M4: Add kind column for MEMORY vs SUMMARY nodes
        if "kind" not in columns:
            self.conn.execute("ALTER TABLE memories ADD COLUMN kind TEXT DEFAULT 'MEMORY'")

        # Backfill t_created from created_at for existing memories
        self.conn.execute("UPDATE memories SET t_created = created_at WHERE t_created IS NULL")

        # M3: Create memory_events table
        self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS memory_events (
                event_id TEXT PRIMARY KEY,
                memory_id TEXT NOT NULL,
                event_type TEXT NOT NULL,
                occurred_at TEXT NOT NULL,
                source TEXT NOT NULL,
                actor TEXT,
                artifact_ref TEXT,
                payload_json TEXT NOT NULL,
                FOREIGN KEY(memory_id) REFERENCES memories(id)
            )
            """
        )

        # M3: Create indexes for temporal queries
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_events_memory_time
            ON memory_events(memory_id, occurred_at DESC)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_events_type
            ON memory_events(event_type)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_memories_expired
            ON memories(t_expired) WHERE t_expired IS NOT NULL
            """
        )
        # M4: Create index for kind column (for summary queries)
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_memories_kind
            ON memories(kind)
            """
        )

        # M4: Create entities table
        self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS entities (
                id TEXT PRIMARY KEY,
                entity_type TEXT NOT NULL,
                canonical_name TEXT NOT NULL,
                norm_key TEXT NOT NULL,
                created_at TEXT NOT NULL,
                expired_at TEXT,
                merged_into TEXT
            )
            """
        )

        # M4: Create indexes for entity queries
        self.conn.execute(
            """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_entities_norm_key
            ON entities(norm_key)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_entities_type
            ON entities(entity_type)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_entities_expired
            ON entities(expired_at) WHERE expired_at IS NOT NULL
            """
        )

        # M4: Create edges table
        self.conn.execute(
            """
            CREATE TABLE IF NOT EXISTS edges (
                edge_id TEXT PRIMARY KEY,
                from_node TEXT NOT NULL,
                to_node TEXT NOT NULL,
                edge_type TEXT NOT NULL,
                weight REAL NOT NULL,
                confidence REAL,
                evidence TEXT,
                t_valid TEXT,
                t_invalid TEXT,
                t_created TEXT,
                t_expired TEXT
            )
            """
        )

        # M4: Create indexes for edge queries
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_edges_from_node
            ON edges(from_node, edge_type)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_edges_to_node
            ON edges(to_node, edge_type)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_edges_type
            ON edges(edge_type)
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_edges_confidence
            ON edges(confidence) WHERE confidence IS NOT NULL
            """
        )
        self.conn.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_edges_expired
            ON edges(t_expired) WHERE t_expired IS NOT NULL
            """
        )
        # M4: UNIQUE constraint to prevent duplicate edges (belt and braces)
        self.conn.execute(
            """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_edges_unique
            ON edges(from_node, to_node, edge_type)
            WHERE t_expired IS NULL
            """
        )

        self.conn.commit()

    def _validate_schema(self) -> None:
        """Validate database schema matches expectations (fail loudly on mismatch)

        Raises:
            RuntimeError: If required tables or columns are missing
        """
        # Check critical columns exist in memories table
        cursor = self.conn.execute("PRAGMA table_info(memories)")
        columns = {row[1] for row in cursor.fetchall()}

        required_columns = {
            "id",
            "content",
            "content_embedding",
            "content_hash",
            "created_at",
            "metadata",
            "t_valid",
            "t_invalid",
            "t_created",
            "t_expired",
            "temporal_stability",
            "last_seen_at",
            "reinforce_count",
            "kind",
        }

        missing = required_columns - columns
        if missing:
            raise RuntimeError(
                f"Schema validation failed: Missing columns in memories table: {missing}. "
                f"Database schema is incompatible with this version of Vestig. "
                f"Expected schema from M4 (Summary Nodes)."
            )

        # Check critical tables exist
        cursor = self.conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = {row[0] for row in cursor.fetchall()}

        required_tables = {"memories", "memory_events", "entities", "edges"}
        missing_tables = required_tables - tables
        if missing_tables:
            raise RuntimeError(f"Schema validation failed: Missing tables: {missing_tables}")

        # Note: Using logger here would require import at top
        # logger.debug("Schema validation passed")

    def store_memory(self, node: MemoryNode, kind: str = "MEMORY") -> str:
        """
        Persist a memory node (M2: handles exact duplicates, M4: supports kind).

        Args:
            node: MemoryNode to store
            kind: Memory kind - "MEMORY" (default) or "SUMMARY"

        Returns:
            Memory ID (existing ID if duplicate detected)
        """
        # Check for exact duplicate via content_hash
        cursor = self.conn.execute(
            "SELECT id FROM memories WHERE content_hash = ?",
            (node.content_hash,),
        )
        existing = cursor.fetchone()

        if existing:
            # Exact duplicate found - return existing ID (nice UX)
            return existing[0]

        # No duplicate - insert new memory
        self.conn.execute(
            """
            INSERT INTO memories (
                id, content, content_embedding, content_hash, created_at, metadata,
                t_valid, t_invalid, t_created, t_expired, temporal_stability,
                last_seen_at, reinforce_count, kind
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                node.id,
                node.content,
                json.dumps(node.content_embedding),
                node.content_hash,
                node.created_at,
                json.dumps(node.metadata),
                node.t_valid,
                node.t_invalid,
                node.t_created,
                node.t_expired,
                node.temporal_stability,
                node.last_seen_at,
                node.reinforce_count,
                kind,
            ),
        )
        # NOTE: Caller manages transaction commit
        return node.id

    def get_memory(self, memory_id: str) -> MemoryNode | None:
        """
        Retrieve a memory by ID.

        Args:
            memory_id: Memory ID to retrieve

        Returns:
            MemoryNode if found, None otherwise
        """
        cursor = self.conn.execute(
            """
            SELECT id, content, content_embedding, content_hash, created_at, metadata,
                   t_valid, t_invalid, t_created, t_expired, temporal_stability,
                   last_seen_at, reinforce_count
            FROM memories
            WHERE id = ?
            """,
            (memory_id,),
        )
        row = cursor.fetchone()
        if row is None:
            return None

        return MemoryNode(
            id=row[0],
            content=row[1],
            content_embedding=json.loads(row[2]),
            content_hash=row[3],
            created_at=row[4],
            metadata=json.loads(row[5]),
            t_valid=row[6],
            t_invalid=row[7],
            t_created=row[8],
            t_expired=row[9],
            temporal_stability=row[10] or "unknown",
            last_seen_at=row[11],
            reinforce_count=row[12] or 0,
        )

    def get_all_memories(self) -> list[MemoryNode]:
        """
        Load all memories (for brute-force search).

        Returns:
            List of all MemoryNode objects
        """
        cursor = self.conn.execute(
            """
            SELECT id, content, content_embedding, content_hash, created_at, metadata,
                   t_valid, t_invalid, t_created, t_expired, temporal_stability,
                   last_seen_at, reinforce_count
            FROM memories
            ORDER BY created_at DESC
            """
        )
        memories = []
        for row in cursor.fetchall():
            memories.append(
                MemoryNode(
                    id=row[0],
                    content=row[1],
                    content_embedding=json.loads(row[2]),
                    content_hash=row[3],
                    created_at=row[4],
                    metadata=json.loads(row[5]),
                    t_valid=row[6],
                    t_invalid=row[7],
                    t_created=row[8],
                    t_expired=row[9],
                    temporal_stability=row[10] or "unknown",
                    last_seen_at=row[11],
                    reinforce_count=row[12] or 0,
                )
            )
        return memories

    def close(self):
        """Close database connection"""
        self.conn.close()

    # M3: Temporal operations

    def increment_reinforce_count(self, memory_id: str) -> None:
        """Increment reinforce_count (convenience cache for TraceRank)"""
        self.conn.execute(
            "UPDATE memories SET reinforce_count = reinforce_count + 1 WHERE id = ?",
            (memory_id,),
        )
        # NOTE: Caller manages transaction commit

    def update_last_seen(self, memory_id: str, timestamp: str) -> None:
        """Update last_seen_at timestamp"""
        self.conn.execute(
            "UPDATE memories SET last_seen_at = ? WHERE id = ?",
            (timestamp, memory_id),
        )
        # NOTE: Caller manages transaction commit

    def deprecate_memory(self, memory_id: str, t_invalid: str | None = None) -> None:
        """Mark memory as deprecated/expired"""
        from datetime import datetime, timezone

        now = datetime.now(timezone.utc).isoformat()
        self.conn.execute(
            """
            UPDATE memories
            SET t_expired = ?, t_invalid = COALESCE(?, t_invalid)
            WHERE id = ?
            """,
            (now, t_invalid, memory_id),
        )
        # NOTE: Caller manages transaction commit

    def get_active_memories(self) -> list[MemoryNode]:
        """Get all non-expired memories (for retrieval)"""
        cursor = self.conn.execute(
            """
            SELECT id, content, content_embedding, content_hash, created_at, metadata,
                   t_valid, t_invalid, t_created, t_expired, temporal_stability,
                   last_seen_at, reinforce_count
            FROM memories
            WHERE t_expired IS NULL
            ORDER BY created_at DESC
            """
        )
        memories = []
        for row in cursor.fetchall():
            memories.append(
                MemoryNode(
                    id=row[0],
                    content=row[1],
                    content_embedding=json.loads(row[2]),
                    content_hash=row[3],
                    created_at=row[4],
                    metadata=json.loads(row[5]),
                    t_valid=row[6],
                    t_invalid=row[7],
                    t_created=row[8],
                    t_expired=row[9],
                    temporal_stability=row[10] or "unknown",
                    last_seen_at=row[11],
                    reinforce_count=row[12] or 0,
                )
            )
        return memories

    def get_summary_for_artifact(self, artifact_ref: str) -> MemoryNode | None:
        """
        Get summary node for a specific artifact (M4).

        Args:
            artifact_ref: Artifact reference (filename/session ID)

        Returns:
            MemoryNode if summary exists, None otherwise
        """
        cursor = self.conn.execute(
            """
            SELECT id, content, content_embedding, content_hash, created_at, metadata,
                   t_valid, t_invalid, t_created, t_expired, temporal_stability,
                   last_seen_at, reinforce_count
            FROM memories
            WHERE kind = 'SUMMARY' AND t_expired IS NULL
            ORDER BY created_at DESC
            """
        )

        for row in cursor.fetchall():
            metadata = json.loads(row[5])
            if metadata.get("artifact_ref") == artifact_ref:
                return MemoryNode(
                    id=row[0],
                    content=row[1],
                    content_embedding=json.loads(row[2]),
                    content_hash=row[3],
                    created_at=row[4],
                    metadata=metadata,
                    t_valid=row[6],
                    t_invalid=row[7],
                    t_created=row[8],
                    t_expired=row[9],
                    temporal_stability=row[10] or "unknown",
                    last_seen_at=row[11],
                    reinforce_count=row[12] or 0,
                )

        return None

    # M4: Entity operations

    def store_entity(self, entity: EntityNode) -> str:
        """
        Persist an entity node (M4: Graph Layer).

        Deduplication via norm_key:
        - If entity with same norm_key exists and not expired, return existing ID
        - Otherwise insert new entity

        Args:
            entity: EntityNode to store

        Returns:
            Entity ID (existing ID if duplicate detected via norm_key)
        """
        # Check for existing entity via norm_key (not expired)
        cursor = self.conn.execute(
            "SELECT id FROM entities WHERE norm_key = ? AND expired_at IS NULL",
            (entity.norm_key,),
        )
        existing = cursor.fetchone()

        if existing:
            # Duplicate found via norm_key - return existing ID
            return existing[0]

        # No duplicate - insert new entity
        self.conn.execute(
            """
            INSERT INTO entities (id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                entity.id,
                entity.entity_type,
                entity.canonical_name,
                entity.norm_key,
                entity.created_at,
                entity.expired_at,
                entity.merged_into,
            ),
        )
        self.conn.commit()
        return entity.id

    def get_entity(self, entity_id: str) -> EntityNode | None:
        """
        Retrieve entity by ID.

        Args:
            entity_id: Entity ID

        Returns:
            EntityNode if found, None otherwise
        """
        cursor = self.conn.execute(
            """
            SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into
            FROM entities
            WHERE id = ?
            """,
            (entity_id,),
        )
        row = cursor.fetchone()

        if not row:
            return None

        return EntityNode(
            id=row[0],
            entity_type=row[1],
            canonical_name=row[2],
            norm_key=row[3],
            created_at=row[4],
            expired_at=row[5],
            merged_into=row[6],
        )

    def find_entity_by_norm_key(
        self, norm_key: str, include_expired: bool = False
    ) -> EntityNode | None:
        """
        Find entity by normalization key (deduplication lookup).

        Args:
            norm_key: Normalization key (format: "TYPE:normalized_name")
            include_expired: Include expired entities in search

        Returns:
            EntityNode if found, None otherwise
        """
        if include_expired:
            query = "SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into FROM entities WHERE norm_key = ?"
        else:
            query = "SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into FROM entities WHERE norm_key = ? AND expired_at IS NULL"

        cursor = self.conn.execute(query, (norm_key,))
        row = cursor.fetchone()

        if not row:
            return None

        return EntityNode(
            id=row[0],
            entity_type=row[1],
            canonical_name=row[2],
            norm_key=row[3],
            created_at=row[4],
            expired_at=row[5],
            merged_into=row[6],
        )

    def get_all_entities(self, include_expired: bool = False) -> list[EntityNode]:
        """
        Get all entities across all types.

        Args:
            include_expired: Include expired entities

        Returns:
            List of EntityNode objects
        """
        if include_expired:
            query = """
                SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into
                FROM entities
                ORDER BY created_at DESC
            """
        else:
            query = """
                SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into
                FROM entities
                WHERE expired_at IS NULL
                ORDER BY created_at DESC
            """

        cursor = self.conn.execute(query)
        entities = []

        for row in cursor.fetchall():
            entities.append(
                EntityNode(
                    id=row[0],
                    entity_type=row[1],
                    canonical_name=row[2],
                    norm_key=row[3],
                    created_at=row[4],
                    expired_at=row[5],
                    merged_into=row[6],
                )
            )

        return entities

    def get_entities_by_type(
        self, entity_type: str, include_expired: bool = False
    ) -> list[EntityNode]:
        """
        Get all entities of a specific type.

        Args:
            entity_type: Entity type (PERSON, ORG, SYSTEM, etc.)
            include_expired: Include expired entities

        Returns:
            List of EntityNode objects
        """
        if include_expired:
            query = """
                SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into
                FROM entities
                WHERE entity_type = ?
                ORDER BY created_at DESC
            """
        else:
            query = """
                SELECT id, entity_type, canonical_name, norm_key, created_at, expired_at, merged_into
                FROM entities
                WHERE entity_type = ? AND expired_at IS NULL
                ORDER BY created_at DESC
            """

        cursor = self.conn.execute(query, (entity_type,))
        entities = []

        for row in cursor.fetchall():
            entities.append(
                EntityNode(
                    id=row[0],
                    entity_type=row[1],
                    canonical_name=row[2],
                    norm_key=row[3],
                    created_at=row[4],
                    expired_at=row[5],
                    merged_into=row[6],
                )
            )

        return entities

    def expire_entity(self, entity_id: str, merged_into: str | None = None) -> None:
        """
        Mark entity as expired (soft delete / merge).

        Args:
            entity_id: Entity ID to expire
            merged_into: Optional ID of entity this was merged into
        """
        from datetime import datetime, timezone

        now = datetime.now(timezone.utc).isoformat()

        self.conn.execute(
            "UPDATE entities SET expired_at = ?, merged_into = ? WHERE id = ?",
            (now, merged_into, entity_id),
        )
        self.conn.commit()

    # M4: Edge operations

    def store_edge(self, edge: EdgeNode) -> str:
        """
        Persist an edge (M4: Graph Layer).

        Args:
            edge: EdgeNode to store

        Returns:
            Edge ID

        Raises:
            ValueError: If edge_type is invalid (enforced in EdgeNode.create())
        """
        # Check for duplicate edge (same from/to/type, not expired)
        cursor = self.conn.execute(
            """
            SELECT edge_id FROM edges
            WHERE from_node = ? AND to_node = ? AND edge_type = ? AND t_expired IS NULL
            """,
            (edge.from_node, edge.to_node, edge.edge_type),
        )
        existing = cursor.fetchone()

        if existing:
            # Duplicate edge found - return existing ID
            return existing[0]

        # Insert new edge
        self.conn.execute(
            """
            INSERT INTO edges (edge_id, from_node, to_node, edge_type, weight,
                              confidence, evidence, t_valid, t_invalid, t_created, t_expired)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                edge.edge_id,
                edge.from_node,
                edge.to_node,
                edge.edge_type,
                edge.weight,
                edge.confidence,
                edge.evidence,
                edge.t_valid,
                edge.t_invalid,
                edge.t_created,
                edge.t_expired,
            ),
        )
        self.conn.commit()
        return edge.edge_id

    def get_edges_from_memory(
        self,
        memory_id: str,
        edge_type: str | None = None,
        include_expired: bool = False,
        min_confidence: float = 0.0,
    ) -> list[EdgeNode]:
        """
        Get all outgoing edges from a memory node.

        Args:
            memory_id: Source memory ID
            edge_type: Optional edge type filter (MENTIONS, RELATED)
            include_expired: Include expired edges
            min_confidence: Minimum confidence threshold (0.0 = all)

        Returns:
            List of EdgeNode objects
        """
        # Build query based on filters
        if edge_type:
            if include_expired:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE from_node = ? AND edge_type = ? AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, edge_type, min_confidence)
            else:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE from_node = ? AND edge_type = ? AND t_expired IS NULL AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, edge_type, min_confidence)
        else:
            if include_expired:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE from_node = ? AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, min_confidence)
            else:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                       confidence, evidence, t_valid, t_invalid, t_created, t_expired
                FROM edges
                WHERE from_node = ? AND t_expired IS NULL AND (confidence IS NULL OR confidence >= ?)
                ORDER BY t_created DESC
                """
                params = (memory_id, min_confidence)

        cursor = self.conn.execute(query, params)
        edges = []

        for row in cursor.fetchall():
            edges.append(
                EdgeNode(
                    edge_id=row[0],
                    from_node=row[1],
                    to_node=row[2],
                    edge_type=row[3],
                    weight=row[4],
                    confidence=row[5],
                    evidence=row[6],
                    t_valid=row[7],
                    t_invalid=row[8],
                    t_created=row[9],
                    t_expired=row[10],
                )
            )

        return edges

    def get_edges_to_entity(
        self,
        entity_id: str,
        include_expired: bool = False,
        min_confidence: float = 0.0,
    ) -> list[EdgeNode]:
        """
        Get all incoming edges to an entity node.

        Args:
            entity_id: Target entity ID
            include_expired: Include expired edges
            min_confidence: Minimum confidence threshold

        Returns:
            List of EdgeNode objects
        """
        if include_expired:
            query = """
                SELECT edge_id, from_node, to_node, edge_type, weight,
                       confidence, evidence, t_valid, t_invalid, t_created, t_expired
                FROM edges
                WHERE to_node = ? AND (confidence IS NULL OR confidence >= ?)
                ORDER BY t_created DESC
            """
        else:
            query = """
                SELECT edge_id, from_node, to_node, edge_type, weight,
                       confidence, evidence, t_valid, t_invalid, t_created, t_expired
                FROM edges
                WHERE to_node = ? AND t_expired IS NULL AND (confidence IS NULL OR confidence >= ?)
                ORDER BY t_created DESC
            """

        cursor = self.conn.execute(query, (entity_id, min_confidence))
        edges = []

        for row in cursor.fetchall():
            edges.append(
                EdgeNode(
                    edge_id=row[0],
                    from_node=row[1],
                    to_node=row[2],
                    edge_type=row[3],
                    weight=row[4],
                    confidence=row[5],
                    evidence=row[6],
                    t_valid=row[7],
                    t_invalid=row[8],
                    t_created=row[9],
                    t_expired=row[10],
                )
            )

        return edges

    def get_edges_to_memory(
        self,
        memory_id: str,
        edge_type: str | None = None,
        include_expired: bool = False,
        min_confidence: float = 0.0,
    ) -> list[EdgeNode]:
        """
        Get all incoming edges to a memory node.

        Args:
            memory_id: Target memory ID
            edge_type: Optional edge type filter (RELATED)
            include_expired: Include expired edges
            min_confidence: Minimum confidence threshold

        Returns:
            List of EdgeNode objects
        """
        # Build query based on filters
        if edge_type:
            if include_expired:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE to_node = ? AND edge_type = ? AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, edge_type, min_confidence)
            else:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE to_node = ? AND edge_type = ? AND t_expired IS NULL AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, edge_type, min_confidence)
        else:
            if include_expired:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE to_node = ? AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, min_confidence)
            else:
                query = """
                    SELECT edge_id, from_node, to_node, edge_type, weight,
                           confidence, evidence, t_valid, t_invalid, t_created, t_expired
                    FROM edges
                    WHERE to_node = ? AND t_expired IS NULL AND (confidence IS NULL OR confidence >= ?)
                    ORDER BY t_created DESC
                """
                params = (memory_id, min_confidence)

        cursor = self.conn.execute(query, params)
        edges = []

        for row in cursor.fetchall():
            edges.append(
                EdgeNode(
                    edge_id=row[0],
                    from_node=row[1],
                    to_node=row[2],
                    edge_type=row[3],
                    weight=row[4],
                    confidence=row[5],
                    evidence=row[6],
                    t_valid=row[7],
                    t_invalid=row[8],
                    t_created=row[9],
                    t_expired=row[10],
                )
            )

        return edges

    def get_edge(self, edge_id: str) -> EdgeNode | None:
        """
        Retrieve edge by ID.

        Args:
            edge_id: Edge ID

        Returns:
            EdgeNode if found, None otherwise
        """
        cursor = self.conn.execute(
            """
            SELECT edge_id, from_node, to_node, edge_type, weight,
                   confidence, evidence, t_valid, t_invalid, t_created, t_expired
            FROM edges
            WHERE edge_id = ?
            """,
            (edge_id,),
        )
        row = cursor.fetchone()

        if not row:
            return None

        return EdgeNode(
            edge_id=row[0],
            from_node=row[1],
            to_node=row[2],
            edge_type=row[3],
            weight=row[4],
            confidence=row[5],
            evidence=row[6],
            t_valid=row[7],
            t_invalid=row[8],
            t_created=row[9],
            t_expired=row[10],
        )

    def expire_edge(self, edge_id: str) -> None:
        """
        Mark edge as expired (soft delete / invalidation).

        Args:
            edge_id: Edge ID to expire
        """
        from datetime import datetime, timezone

        now = datetime.now(timezone.utc).isoformat()

        self.conn.execute(
            "UPDATE edges SET t_expired = ? WHERE edge_id = ?",
            (now, edge_id),
        )
        self.conn.commit()


---
src/vestig/core/tracerank.py
---
"""TraceRank: Temporal reinforcement scoring for M3"""

import math
from dataclasses import dataclass
from datetime import datetime, timezone

from vestig.core.models import EventNode


@dataclass
class TraceRankConfig:
    """TraceRank configuration"""

    enabled: bool = True
    tau_days: float = 21.0  # Recency decay time constant (3 weeks)
    cooldown_hours: float = 24.0  # Anti-burst window
    burst_discount: float = 0.2  # Weight for events in cooldown
    k: float = 0.35  # TraceRank boost strength

    # Enhanced force multiplier parameters
    graph_connectivity_enabled: bool = True  # Boost based on inbound edges
    graph_k: float = 0.15  # Graph connectivity boost strength
    temporal_decay_enabled: bool = True  # Apply age-based decay for dynamic facts
    dynamic_tau_days: float = 90.0  # Decay time for dynamic facts (3 months)
    ephemeral_tau_days: float | None = None  # Optional override for ephemeral decay
    static_boost: float = 1.0  # Multiplier for static facts (no decay)


def compute_tracerank_multiplier(
    events: list[EventNode], config: TraceRankConfig, query_time: datetime | None = None
) -> float:
    """
    Compute TraceRank multiplier from reinforcement events.

    Algorithm:
    1. For each event, compute recency weight: exp(-Δt / τ)
    2. Apply burst discount if event within cooldown of previous
    3. Sum weighted contributions: trace = Σ(w_recency * w_burst)
    4. Convert to multiplier: 1 + k * log1p(trace)

    Args:
        events: List of EventNode objects (should be REINFORCE_* events)
        config: TraceRankConfig with algorithm parameters
        query_time: Time to compute recency from (default: now)

    Returns:
        Multiplier ∈ [1.0, ∞) to boost semantic similarity

    Examples:
        >>> config = TraceRankConfig(enabled=True, tau_days=21.0, k=0.35)
        >>> # No events → multiplier = 1.0 (no boost)
        >>> compute_tracerank_multiplier([], config)
        1.0

        >>> # One recent event → slight boost
        >>> # Multiple spaced events → higher boost
        >>> # Burst events (within 24h) → discounted
    """
    if not config.enabled or not events:
        return 1.0

    if query_time is None:
        query_time = datetime.now(timezone.utc)

    trace = 0.0
    prev_event_time = None

    # Events should be sorted newest first (get_reinforcement_events does this)
    for event in events:
        # Parse event timestamp
        event_time = datetime.fromisoformat(event.occurred_at.replace("Z", "+00:00"))

        # Recency decay: exp(-Δt / τ)
        delta_days = (query_time - event_time).total_seconds() / 86400
        w_recency = math.exp(-delta_days / config.tau_days)

        # Anti-burst: discount events within cooldown
        w_burst = 1.0
        if prev_event_time is not None:
            gap_hours = (prev_event_time - event_time).total_seconds() / 3600
            if gap_hours < config.cooldown_hours:
                w_burst = config.burst_discount

        trace += w_recency * w_burst
        prev_event_time = event_time

    # Convert trace to multiplier: 1 + k * log1p(trace)
    # log1p(x) = log(1+x) is numerically stable for small x
    multiplier = 1.0 + config.k * math.log1p(trace)

    return multiplier


def compute_graph_connectivity_boost(inbound_edge_count: int, config: TraceRankConfig) -> float:
    """
    Compute boost multiplier based on graph connectivity.

    Memories with more inbound edges (RELATED, MENTIONS) are more central
    to the knowledge graph and may be more relevant.

    Algorithm:
    - boost = 1 + graph_k * log1p(edge_count)
    - More edges → higher boost (logarithmic growth)

    Args:
        inbound_edge_count: Number of edges pointing to this memory
        config: TraceRankConfig with graph_k parameter

    Returns:
        Multiplier ∈ [1.0, ∞) based on connectivity

    Examples:
        >>> config = TraceRankConfig(graph_connectivity_enabled=True, graph_k=0.15)
        >>> # No edges → multiplier = 1.0
        >>> compute_graph_connectivity_boost(0, config)
        1.0
        >>> # 5 edges → slight boost
        >>> compute_graph_connectivity_boost(5, config)  # ~1.27
        >>> # 20 edges → stronger boost
        >>> compute_graph_connectivity_boost(20, config)  # ~1.46
    """
    if not config.graph_connectivity_enabled or inbound_edge_count <= 0:
        return 1.0

    # Logarithmic boost: 1 + graph_k * log1p(edge_count)
    boost = 1.0 + config.graph_k * math.log1p(inbound_edge_count)
    return boost


def compute_temporal_confidence(
    t_valid: str,
    temporal_stability: str,
    config: TraceRankConfig,
    query_time: datetime | None = None,
) -> float:
    """
    Compute confidence multiplier based on temporal stability and age.

    Dynamic facts degrade over time (older = less confident).
    Static facts don't degrade (permanent truths).

    Algorithm:
    - If static: multiplier = static_boost (default 1.0, no decay)
    - If dynamic: multiplier = exp(-age_days / dynamic_tau_days)
    - If ephemeral: multiplier = exp(-age_days / ephemeral_tau_days)
    - If unknown: multiplier = 1.0 (neutral)

    Args:
        t_valid: When the fact became true (ISO 8601)
        temporal_stability: "static" | "dynamic" | "ephemeral" | "unknown"
        config: TraceRankConfig with temporal decay parameters
        query_time: Time to compute age from (default: now)

    Returns:
        Confidence multiplier ∈ (0.0, 1.0] for dynamic facts, 1.0 for static

    Examples:
        >>> config = TraceRankConfig(temporal_decay_enabled=True, dynamic_tau_days=90.0)
        >>> # Static fact, any age → no decay
        >>> compute_temporal_confidence("2020-01-01", "static", config)
        1.0
        >>> # Dynamic fact, 1 year old → significant decay
        >>> compute_temporal_confidence("2024-01-01", "dynamic", config)  # ~0.025
        >>> # Dynamic fact, 1 day old → minimal decay
        >>> compute_temporal_confidence("2025-12-26", "dynamic", config)  # ~0.99
    """
    if not config.temporal_decay_enabled:
        return 1.0

    # Static facts don't decay
    if temporal_stability == "static":
        return config.static_boost

    # Unknown stability → neutral (no decay)
    if temporal_stability == "unknown":
        return 1.0

    # Dynamic/ephemeral facts: exponential decay based on age
    if query_time is None:
        query_time = datetime.now(timezone.utc)

    try:
        t_valid_dt = datetime.fromisoformat(t_valid.replace("Z", "+00:00"))
        age_days = (query_time - t_valid_dt).total_seconds() / 86400

        # Only apply decay for positive age (past facts)
        if age_days > 0:
            tau_days = config.dynamic_tau_days
            if temporal_stability == "ephemeral":
                tau_days = config.ephemeral_tau_days or max(3.0, min(7.0, tau_days / 4))

            # Exponential decay: exp(-age / tau)
            confidence = math.exp(-age_days / tau_days)
            return max(0.01, confidence)  # Floor at 1% confidence

        # Future facts or same-day facts: no decay
        return 1.0

    except (ValueError, AttributeError):
        # Malformed t_valid → neutral confidence
        return 1.0


def compute_enhanced_multiplier(
    memory_id: str,
    temporal_stability: str,
    t_valid: str,
    inbound_edge_count: int,
    reinforcement_events: list[EventNode],
    config: TraceRankConfig,
    query_time: datetime | None = None,
) -> float:
    """
    Compute comprehensive force multiplier incorporating all factors.

    final_multiplier = reinforcement_boost × graph_boost × temporal_confidence

    Args:
        memory_id: Memory ID (for logging/debugging)
        temporal_stability: "static" | "dynamic" | "ephemeral" | "unknown"
        t_valid: When fact became true (ISO 8601)
        inbound_edge_count: Number of edges pointing to this memory
        reinforcement_events: List of reinforcement events
        config: TraceRankConfig with all parameters
        query_time: Time to compute from (default: now)

    Returns:
        Comprehensive multiplier to apply to semantic similarity

    Example:
        >>> config = TraceRankConfig()
        >>> # Dynamic fact: 30 days old, 5 edges, 2 reinforcements
        >>> compute_enhanced_multiplier(
        ...     "mem_123", "dynamic", "2025-11-27", 5, [...], config
        ... )
        # Returns ~1.5 (reinforcement × graph × temporal_decay)
    """
    # Component 1: Reinforcement boost (existing TraceRank)
    reinforcement_boost = compute_tracerank_multiplier(
        reinforcement_events, config, query_time
    )

    # Component 2: Graph connectivity boost
    graph_boost = compute_graph_connectivity_boost(inbound_edge_count, config)

    # Component 3: Temporal confidence (decay for dynamic facts)
    temporal_confidence = compute_temporal_confidence(
        t_valid, temporal_stability, config, query_time
    )

    # Combine all factors
    final_multiplier = reinforcement_boost * graph_boost * temporal_confidence

    return final_multiplier


---
